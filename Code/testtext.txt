Computer graphics means drawing pictures on a computer screen. What's so good about that? Sketch something on paper—a man or a house—and what you have is a piece of analog information: the thing you draw is a likeness or analogy of something in the real world. Depending on the materials you use, changing what you draw can be easy or hard: you can erase pencil or charcoal marks easily enough, and you can scrape off oil paints and redo them with no trouble; but altering watercolors or permanent markers is an awful lot more tricky. That's the wonder of art, of course—it captures the fresh dash of creativity—and that's exactly what we love about it. But where everyday graphics is concerned, the immediacy of art is also a huge drawback. As every sketching child knows too well, if you draw the first part of your picture too big, you'll struggle to squeeze everything else on the page.... and what if you change your mind about where to put something or you want to swap red for orange or green for blue? Ever had one of those days where you rip up sheet after sheet of spoiled paper and toss it in the trash? That's why many artists, designers, and architects have fallen in love with computer graphics. Draw a picture on a computer screen and what you have is a piece of digital information. It probably looks similar to what you'd have drawn on paper—the ghostly idea that was hovering in your mind's eye to begin with—but inside the computer your picture is stored as a series of numbers. Change the numbers and you can change the picture, in the blink of an eye or even quicker. It's easy to shift your picture around the screen, scale it up or down, rotate it, swap the colors, and transform it in all kinds of other ways. Once it's finished, you can save it, incorporate it into a text document, print it out, upload it to a web page, or email it to a client or work colleague—all because it's digital information. There's an alternative method of computer graphics that gets around the problems of raster graphics. Instead of building up a picture out of pixels, you draw it a bit like a child would by using simple straight and curved lines called vectors or basic shapes (circles, curves, triangles, and so on) known as primitives. With raster graphics, you make a drawing of a house by building it from hundreds, thousands, or millions of individual pixels; importantly, each pixel has no connection to any other pixel except in your brain. With vector graphics, you might draw a rectangle for the basic house, smaller rectangles for the windows and door, a cylinder for the smokestack, and a polygon for the roof. Staring at the screen, a vector-graphic house still seems to be drawn out of pixels, but now the pixels are precisely related to one another—they're points along the various lines or other shapes you've drawn. Drawing with straight lines and curves instead of individual dots means you can produce an image more quickly and store it with less information: you could describe a vector-drawn house as "two red triangles and a red rectangle (the roof) sitting on a brown rectangle (the main building)," but you couldn't summarize a pixelated image so simply. It's also much easier to scale a vector-graphic image up and down by applying mathematical formulas called algorithms that transform the vectors from which your image is drawn. That's how computer programs can scale fonts to different sizes without making them look all pixelated and grainy. Real life isn't like a computer game or a virtual reality simulation. The very best CGI (computer-generated imagery) animations are easy to tell apart from ones made on film or video with real actors. Why is that? When we look at objects in the world around us, they don't appear to be drawn from either pixels or vectors. In the blink of an eye, our brains gather much more information from the real-world than artists can include in even the most realistic computer-graphic images. To make a computerized image look anything like as realistic as a photograph (let alone a real-world scene), we need to include far more than simply millions of colored-in pixels. Really sophisticated computer graphics programs use a whole series of techniques to make hand-drawn (and often completely imaginary) two-dimensional images look at least as realistic as photographs. The simplest way of achieving this is to rely on the same tricks that artists have always used—such things as perspective (how objects recede into the distance toward a "vanishing point" on the horizon) and hidden-surface elimination (where nearby things partly obscure ones that are further away).
If you want realistic 3D artwork for such things as CAD (computer-aided design) and virtual reality, you need much more sophisticated graphic techniques. Rather than drawing an object, you make a 3D computer model of it inside the computer and manipulate it on the screen in various ways. First, you build up a basic three-dimensional outline of the object called a wire-frame (because it's drawn from vectors that look like they could be little metal wires). Then the model is rigged, a process in which different bits of the object are linked together a bit like the bones in a skeleton so they move together in a realistic way. Finally, the object is rendered, which involves shading the outside parts with different textures (surface patterns), colors, degrees of opacity or transparency, and so on. Rendering is a hugely complex process that can take a powerful computer hours, days, or even weeks to complete. Sophisticated math is used to model how light falls on the surface, typically using either ray tracing (a relatively simple method of plotting how light bounces off the surface of shiny objects in straight lines) or radiosity (a more sophisticated method for modeling how everyday objects reflect and scatter light in duller, more complex ways).
Obvious uses of computer graphics include computer art, CGI films, architectural drawings, and graphic design—but there are many non-obvious uses as well and not all of them are "artistic." Scientific visualization is a way of producing graphic output from computer models so it's easier for people to understand. Computerized models of global warming produce vast tables of numbers as their output, which only a PhD in climate science could figure out; but if you produce a speeded-up animated visualization—with the Earth getting bluer as it gets colder and redder as it gets hotter—anyone can understand what's going on. Medical imaging is another good example of how graphics make computer data more meaningful. When doctors show you a brain or body scan, you're looking at a computer graphic representation drawn using vast amounts of data produced from thousands or perhaps even millions of measurements. The jaw-dropping photos beamed back from space by amazing devices like the Hubble Space Telescope are usually enhanced with the help of a type of computer graphics called image processing; that might sound complex, but it's not so very different from using a graphics package like Google Picasa or PhotoShop to touch up your holiday snaps).
And that's really the key point about computer graphics: they turn complex computer science into everyday art we can all grasp, instantly and intuitively. Back in the 1980s when I was programming a Commodore PET, the only way to get it to do anything was to type meaningless little words like PEEK and POKE onto a horribly unfriendly green and black screen. Virtually every modern computer now has what's called a GUI (graphical user interface), which means you operate the machine by pointing at things you want, clicking on them with your mouse or your finger, or dragging them around your "desktop." It makes so much more sense because we're visual creatures: something like a third of our cortex (higher brain) is given over to processing information that enters our heads through our eyes. That's why a picture really is worth a thousand words (sometimes many more) and why computers that help us visualize things with computer graphics have truly revolutionized the way we see the world.
Data Science is the theory and practice powering the data-driven transformations we are seeing across industry and society today.
From Artificial Intelligence machine learning to self-driving cars and predictive analytics are just of the few breakthroughs have been made thanks to our ever-growing ability to collect and analyse data.
Just as with Big Data and Artificial Intelligence the field of Data Science has developed its own lexicon which can be confusing at first for beginners. An understanding of the basic terminology and frequently used terms is essential for anyone thinking about how this technology can be applied. So here is my run through of some of the technologies, phrases and buzzwords you are likely to come across.
Anonymization When carrying out scientific data analysis using personal data (data which identifies a person), anonymization refers to the process of removing or obfuscating indicators in the data which show who it specifically refers to. This is not always as simple as it sounds as people can be identified by more than just their name. Properly anonymized data is no longer considered “personal” and there are commonly less legal and ethical restrictions on how it can be used.
Algorithm Repeatable sets of instructions which people or machines can use to process data. Typically, algorithms are constructed by feeding data into them and adjusting variables until a desired outcome is achieved. Thanks to breakthroughs in Artificial intelligence such as machine learning and neural networks today machines generally do this, as they can do it far more quickly than any human.
Artificial Intelligence Today’s AIs are built on concepts developed through the study and application of data science. One way to categorize the latest wave of “intelligent” machines is as machines which are capable of carrying out data science for themselves. Rather than simply process the data they are fed, in the way they are told, they can learn it and adapt to become better at processing it. This is how Google Translate becomes better at understanding language, and how autonomous cars will navigate areas they have never visited before.
Bayes Theorem A mathematical formula used to predict the probability of one event occurring in relation to whether or not another event has occurred. It is a commonly used technique used in data science to establish probabilities and outcomes which are dependent on unknown variables, and is used to build Bayesian Networks, where the principle is applied across large datasets.
Behavioural analytics The use of data on a person or object’s behaviour to make predictions on how it might change in the future (see predictive modelling in Part II) or determining variables which affect it, so more favourable or efficient outcomes might be achieved.
Big Data Big Data is the “buzzword” term which has come to represent the vast increase in the amount of data which has become available in recent years, particularly as the world has increasingly become online and connected through the internet. This data is distinguished from data previously available not just by its size but also the high speed at which it is generated, and the large variations in the forms it can take. It greatly expands the potential of what can be achieved with data science, which was previously hampered by slow computer processing speeds and the difficulty of capturing accurate information in large volumes, before the widespread digitization.
Citizen Data Scientist Sometimes also referred to as an “armchair data scientist”. One of the increasing number of people who although not academically trained or professionally employed primarily as data scientists, are able to use data science tools and techniques to improve the use of information in their own field of study or work. This is increasingly becoming possible thanks to the growing number of automated or “self service” tools and platforms for data analytics.
Classification The ability to use data (about an object, event or anything else) to determine which of a number of predetermined groups an item belongs in. For a basic example, an image recognition analysis might classify all shapes with four equal sides as squares, and all shapes with three sides as triangles.
Clickstream analytics Analysis of the way humans interact with computers or use machinery – the name refers to recording and analyzing where a mouse is clicked on a screen (with the sequence of interactive actions taken by the user known as the “clickstream”) but it can be apply to any method of interaction that can be measured – such as manual operation of machinery using a joystick or control panel, or voice recognition.
Clustering Clustering is also about grouping objects together but it differs because it is used when there are no predetermined groups. Objects (or events) are clustered together due to similarities they share and algorithms determine what that common relationship between them may be. Clustering is a data science technique which makes unsupervised learning possible.
Data Governance Rules which establish how data should be used, in order to both comply with legislation and ensure the integrity of data and data-driven initiatives.
Data Mining The process of examining a set of data to determine relationships between variables which could affect outcomes – generally at large scale and by machines. Data mining is an older term used by computer scientists and in business to describe the basic function of a data scientist or a data science initiative.
Data Set The entire collection of data that will be used in a particular data science initiative. In modern complex Big Data projects, it can involve many types of data gathered from different sources.
Data Scientist A person who applies the scientific method of observing, recording, analysing and reporting results to understand information and use it to solve problems.
Democratization of Data Science The idea that data science tools and techniques are increasingly accessible to a growing number of people, rather than only those in academia or industry with access to large budgets. See also Citizen Data Scientist.
Decision Trees A basic decision-making structure which can be used by a computer to understand and classify information. By asking a series of questions about each data item fed into them, outputs are channeled along different branches leading to different outcomes, typically labelling or classification of the piece of data.
Dimension Data can be stored in a database which has one dimension – a list, or two dimensions – a grid made up of rows and columns. It can also be stored in multi-dimensional databases which can take the form of a grid, with three axes, or even more complex permutations, which are not possible to relate to common geospatial objects, thanks to the power of CPU processing. More complex dimensional structures typically allow for more connections to be observed between the data objects which are being analyzed.
In-Memory Database A database which is held in a computer’s RAM memory where it can be accessed and operated far more quickly than if the data is read from a disk whenever it needs to be accessed. This is something which has become possible in recent years while it was very difficult to do with large data sets in the past, due to the increase in size of available memory, and the fall in the cost of physical RAM chips.
Metadata Data about data, or data attached to other data – for example with an image file this would be information about its size, when it was created, what camera was used to take it, or which version of a software package it was created in.
Outlier A variable where the value is very different from that which is expected considering the value of other variables in the dataset. These can be indicators of rare or unexpected events, or of unreliable data.
Predictive Modelling Using data to predict the future. Rather than a crystal ball or tealeaves, data scientists use probability and statistics to determine what is most likely to happen next. The more data that is available from past events, the more likely that algorithms can give a prediction with a high probability of proving correct. Predictive modelling involves running a large number of simulated events in order to determine the variables most likely to produce a desired outcome.
Python Python is a programming language which has become highly popular with data scientists in recent years due to its relative ease of use and the sophisticated ways it can be used to work with large, fast-moving datasets. Its open source (anyone can add to it or change it) nature means its capabilities are constantly being expanded, and new resources are becoming available.
Quantile A group of objects which have been classified according to similar characteristics, and then distributed evenly between a number of such groups. These are distinguished as “quartile” if there are four such groups, “quintile” if there are five such groups, etc. The “first quartile” would refer to the top quarter of entries in a list which has been split into four equal groups.
R Another programming language which has been around for longer than Python and traditionally was the choice for statisticians working with large data sets is R. Although Python is quickly gaining in popularity R is still heavily used by data scientists and is commonly taught on data science courses at universities.
Random Forest A random forest is a method of statistical analysis which involves taking the output of a large number of decision trees (see above) and analyzing them together, to provide a more complex and detailed understanding or classification of data than would be possible with just one tree. As with decision trees this is a technique that has been around in statistics for a long time but modern computers allow for far more complex trees and forests, leading to more accurate predictions.
Standard Deviation A common calculation in data science used to measure how far removed a variable, statistic or measurement is from the average. This can be used to determine how closely a piece of data fits to the norm of whatever it represents (speed of movement, temperature of a piece of machinery, population size of a developed area) and allows inferences to be made on why it differs from the norm.
algorithms and data structure machine learning artificial intelligence 
Virtual Reality (VR) is the use of computer technology to create a simulated environment. Unlike traditional user interfaces, VR places the user inside an experience. Instead of viewing a screen in front of them, users are immersed and able to interact with 3D worlds. By simulating as many senses as possible, such as vision, hearing, touch, even smell, the computer is transformed into a gatekeeper to this artificial world. The only limits to near-real VR experiences are the availability of content and cheap computing power.
Virtual Reality and Augmented Reality are two sides of the same coin. You could think of Augmented Reality as VR with one foot in the real world: Augmented Reality simulates artificial objects in the real environment; Virtual Reality creates an artificial environment to inhabit.
In Augmented Reality, the computer uses sensors and algorithms to determine the position and orientation of a camera. AR technology then renders the 3D graphics as they would appear from the viewpoint of the camera, superimposing the computer-generated images over a user’s view of the real world.
In Virtual Reality, the computer uses similar sensors and math. However, rather than locating a real camera within a physical environment, the position of the user’s eyes are located within the simulated environment. If the user’s head turns, the graphics react accordingly. Rather than compositing virtual objects and a real scene, VR technology creates a convincing, interactive world for the user.
Virtual Reality’s most immediately-recognizable component is the head-mounted display (HMD). Human beings are visual creatures, and display technology is often the single biggest difference between immersive Virtual Reality systems and traditional user interfaces. For instance, CAVE automatic virtual environments actively display virtual content onto room-sized screens. While they are fun for people in universities and big labs, consumer and industrial wearables are the wild west.
With a multiplicity of emerging hardware and software options, the future of wearables is unfolding but yet unknown. Concepts such as the HTC Vive Pro Eye, Oculus Quest and Playstation VR are leading the way, but there are also players like Google, Apple, Samsung, Lenovo and others who may surprise the industry with new levels of immersion and usability. Whomever comes out ahead, the simplicity of buying a helmet-sized device that can work in a living-room, office, or factory floor has made HMDs center stage when it comes to Virtual Reality technologies.
Computer animation is the art of creating moving images via the use of computers.
It is a subfield of computer graphics and animation.
Increasingly it is created by means of 3D computer graphics, though 2D computer graphics are still widely used for low bandwidth and faster real-time rendering needs.
Sometimes the target of the animation is the computer itself, but it sometimes the target is another medium, such as film.
It is also referred to as CGI (Computer-generated imagery or computer-generated imaging), especially when used in films.
To create the illusion of movement, an image is displayed on the computer screen then quickly replaced by a new image that is similar to the previous image, but shifted slightly.
This technique is identical to how the illusion of movement is achieved with television and motion pictures.
Computer animation is essentially a digital successor to the art of stop motion animation of 3D models and frame-by-frame animation of 2D illustrations.
For 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton.
For 2D figure animations, separate objects (illustrations) and separate transparent layers are used, with or without a virtual skeleton.
Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames.
The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing.
Finally, the animation is rendered.
For 3D animations, all frames must be rendered after modeling is complete.
For 2D vector animations, the rendering process is the key frame illustration process, while tweened frames are rendered as needed.
For pre-recorded presentations, the rendered frames are transferred to a different format or medium such as film or digital video.
The frames may also be rendered in real time as they are presented to the end-user audience.
Low bandwidth animations transmitted via the internet (e.g. 2D Flash, X3D) often use software on the end-users computer to render in real time as an alternative to streaming or pre-loaded high bandwidth animations.
Information Security Management (ISM) ensures confidentiality, authenticity, non-repudiation, integrity, and availability of organization data and IT services. It also ensures reasonable use of organization’s information resources and appropriate management of information security risks.
Information Security Manager is the process owner of this process.
Information security is considered to be met when 
Information is observed or disclosed on only authorized persons
Information is complete, accurate and protected against unauthorized access (integrity)
Information is available and usable when required, and the systems providing the information resist attack and recover from or prevent failures (availability)
Business transaction as well information exchanges between enterprises, or with partners, can be trusted (authenticity and non-repudiation)
Information security, sometimes abbreviated to infosec, is a set of practices intended to keep data secure from unauthorized access or alterations, both when it's being stored and when it's being transmitted from one machine or physical location to another. You might sometimes see it referred to as data security. As knowledge has become one of the 21st century's most important assets, efforts to keep information secure have correspondingly become increasingly important.
Because information technology has become the accepted corporate buzzphrase that means, basically, "computers and related stuff," you will sometimes see information security and cybersecurity used interchangeably. Strictly speaking, cybersecurity is the broader practice of defending IT assets from attack, and information security is a specific discipline under the cybersecurity umbrella. Network security and application security are sister practices to infosec, focusing on networks and app code, respectively.
Obviously, there's some overlap here. You can't secure data transmitted across an insecure network or manipulated by a leaky application. As well, there is plenty of information that isn't stored electronically that also needs to be protected. Thus, the infosec pro's remit is necessarily broad.
The basic components of information security are most often summed up by the so-called CIA triad: confidentiality, integrity, and availability.
Confidentiality is perhaps the element of the triad that most immediately comes to mind when you think of information security. Data is confidential when only those people who are authorized to access it can do so; to ensure confidentiality, you need to be able to identify who is trying to access data and block attempts by those without authorization. Passwords, encryption, authentication, and defense against penetration attacks are all techniques designed to ensure confidentiality.
Integrity means maintaining data in its correct state and preventing it from being improperly modified, either by accident or maliciously. Many of the techniques that ensure confidentiality will also protect data integrity—after all, a hacker can't change data they can't access—but there are other tools that help provide a defense of integrity in depth: checksums can help you verify data integrity, for instance, and version control software and frequent backups can help you restore data to a correct state if need be. Integrity also covers the concept of non-repudiation: you must be able to prove that you've maintained the integrity of your data, especially in legal contexts.
Availability is the mirror image of confidentiality: while you need to make sure that your data can't be accessed by unauthorized users, you also need to ensure that it can be accessed by those who have the proper permissions. Ensuring data availability means matching network and computing resources to the volume of data access you expect and implementing a good backup policy for disaster recovery purposes.
In an ideal world, your data should always be kept confidential, in its correct state, and available; in practice, of course, you often need to make choices about which information security principles to emphasize, and that requires assessing your data. If you're storing sensitive medical information, for instance, you'll focus on confidentiality, whereas a financial institution might emphasize data integrity to ensure that nobody's bank account is credited or debited incorrectly.
An embedded system is a combination of computer hardware and software designed for a specific function or functions within a larger system. The systems can be programmable or with fixed functionality. Industrial machines, consumer electronics, agricultural and process industry devices, automobiles, medical equipment, cameras, household appliances, airplanes, vending machines and toys, as well as mobile devices, are possible locations for an embedded system
The main characteristic of embedded systems is that they are task specific. They perform a single task within a larger system. For example, a mobile phone is not an embedded system, it is a combination of embedded systems that together allow it to perform a variety of general-purpose tasks. The embedded systems within it perform specialized functions. For example, the GUI performs the singular function of allowing the user to interface with the device. In short, they are programmable computers, but designed for specific purposes, not general ones.
Additionally, embedded systems can include the following characteristics:
comprised of hardware, software and firmware;
embedded in a larger system to perform a specific function as they are built for specialized tasks within the system, not various tasks;
either microprocessor-based or microcontroller-based both are integrated circuits that give the system compute power;
often used for sensing and real-time computing in internet of things (IoT) devices -- devices that are internet-connected and do not require a user to operate;
vary in complexity and in function, which affects the type of software, firmware and hardware they use; and
often required to perform their function under a time constraint to keep the larger system functioning properly.
While cryptocurrencies have seen their values spike and plummet, they still see comparatively few transactions for everyday use. The number of businesses which accept them is limited, while the transaction costs for bitcoin become too high when the currency is frequently used. On top of this, the bitcoin network can’t handle anywhere near the volume of transactions as an alternative like the traditional Visa system.
While there are cryptocurrencies that seem more promising than bitcoin, these are accepted in even fewer places. At this stage, it seems like the main uses for cryptocurrencies are as speculative investments or to buy illicit products from darknet marketplaces.
Before we dive in too deep, it’s important to cover the basics. Cryptography is the study and practice of keeping secret information away from adversaries. In the early days, it was done simply, using techniques such as changing each letter in a word to the letter that follows it in the alphabet. Under this type of scheme:
Bitcoin was initially proposed as a cryptography-based currency that could avoid the downsides of having a financial system controlled by central institutions.
At the core of bitcoin is the idea of transferring value through a chain of digital signatures, which are similar to handwritten signatures.
The non-banking financial institutions’ (NFIs’) sector has been recently regulated in Romania. The National Bank of Romania (NBR) proposed and the Parliament passed a series of provisions regarding the activities carried out by these entities in order to strengthen the stability of the financial sector as a whole. According to these provisions, the NBR is obliged to prudentially supervise the most important NFIs: those register in the NBR’ Special register. The other NFIs are monitored and only in special cases are further scrutinized. However, all NFIs have to report on their activity to the Supervision Department. One such reporting consists of periodic financial statements (PFSs) that a NFI has to sent to NBR quarterly. Currently, these PFSs are analysed manually by the inspectors from the department. Based on their assessment, NFIs that present difficulties are further scrutinized and, eventually, an on-site inspection is organized. The scarce time and personnel resources of the Supervision Department and the need to balance the subjective interpretation of the inspectors motivate the use of some sorts of techniques that would classify the NFIs as having a “good” or “poor” performance. Based on these techniques we can build so-called classification models that might provide additional help in taking the decision for an on-site inspection. Data Mining techniques (Han & Kamber, 2006) can help with building the clustering/classification models. Clustering techniques can be used to find performance clusters within the NFIs’ performance dataset and classification techniques can be used to place a new NFI into a predefined cluster as data become available. In other words, clustering techniques have descriptive properties and classification techniques have prescriptive ones. In this article we apply the Fuzzy C-Means algorithm (Bezdek, 1981) and obtaining clusters with similar performance. We attach to each input dataset observation a performance class depending on which cluster contains the observation given the characterization and hierarchy of the clusters in “good”, “medium” and “poor” performance clusters. Finally, we apply artificial neural networks (ANNs) trained with genetic algorithms in order to find a function that maps the input performance space on the newly constructed performance class variable. In this way we overcome the problem of accommodating new NFIs on the performance map as data become available. Next, we present in more detail our methodology. Then, we present the NFIs’ performance dataset and our experiment on applying the Data Mining techniques on these data. Finally, we draw our conclusions.
In this study we train artificial neural networks with genetic algorithms in order to obtain performance classification models that might be used to find the future performance of non-banking financial institutions in Romania. As a first phase of our methodology we applied a clustering technique (FCM algorithm) in order to group NFIs with similar performances. Then, using initialization, selection and reproduction mechanisms we trained a neural network based on genetic learning. A brief rationale supported with references is given regarding the parameters’ setting for the genetic algorithm. We obtained high training and testing accuracy rates for the classifier and small differences between training and testing accuracy rates. The reproduction of the process of natural selection (which forms the base for the genetic learning) in addressing real problems is welcomed. However, the results have to be taken with some precaution, given the genetic algorithms take long time to learn compared to other computational techniques such as 
Agent Oriented Software Engineering is the one of the most recent contributions to the field of Software Engineering. It has several benefits compared to existing development approaches, in particular the ability to let agents represent high-level abstractions of active entities in a software system. This paper gives an overview of recent research and industrial applications of both general high-level methodologies and on more specific design methodologies for industry-strength software engineering.
Agent oriented software engineering construction of intelligent systems by the use of the agent paradigm, that is, using agent-oriented notions, in any high-level, programming language. In a strict sense: the study of the implementation of agent systems by means of agent oriented programming languages.
Agent Oriented Software Engineering is being described as a new paradigm [22] for the researchfield ofSoftware Engineering.But in order tobecome a new paradigm for the software industry,robust and easy-to-use methodologies and toolshave to be developed.But first, let us explain what an agent is. An agent,also called a software agent or an intelligent agent,is a piece of autonomous software, the words intel-ligent and agent describe some of its characteristicfeatures. Intelligent is used because the software canhave certain types of behavior (“Intelligent behavioris the selection of actions based on knowledge”), andthe termagenttells something about the purpose ofthe software. An agent is“one who is authorized toact for or in the place of another”(Merriam Web-ster’s Dictionary). 
The main purposes of Agent-Oriented Software En-gineering are to create methodologies and tools thatenables inexpensive development and maintenanceof agent-based software. In addition, the softwareshould be flexible, easy-to-use, scalable [5] and ofhigh quality. In other words quite similar to the re-search issues of other branches of software engineer-ing, e.g. object-oriented software engineering 
Computer-aided design (CAD) involves creating computer models defined by geometrical parameters. These models typically appear on a computer monitor as a three-dimensional representation of a part or a system of parts, which can be readily altered by changing relevant parameters. CAD systems enable designers to view objects under a wide variety of representations and to test these objects by simulating real-world conditions.
Computer-aided manufacturing (CAM) uses geometrical design data to control automated machinery. CAM systems are associated with computer numerical control (CNC) or direct numerical control (DNC) systems. These systems differ from older forms of numerical control (NC) in that geometrical data are encoded mechanically. Since both CAD and CAM use computer-based methods for encoding geometrical data, it is possible for the processes of design and manufacture to be highly integrated. Computer-aided design and manufacturing systems are commonly referred to as CAD/CAM.
CAD had its origins in three separate sources, which also serve to highlight the basic operations that CAD systems provide. The first source of CAD resulted from attempts to automate the drafting process. These developments were pioneered by the General Motors Research Laboratories in the early 1960s. One of the important time-saving advantages of computer modeling over traditional drafting methods is that the former can be quickly corrected or manipulated by changing a model's parameters. The second source of CAD was in the testing of designs by simulation. The use of computer modeling to test products was pioneered by high-tech industries like aerospace and semiconductors. The third source of CAD development resulted from efforts to facilitate the flow from the design process to the manufacturing process using numerical control (NC) technologies, which enjoyed widespread use in many applications by the mid-1960s. It was this source that resulted in the linkage between CAD and CAM. One of the most important trends in CAD or CAM technologies is the ever-tighter integration between the design and manufacturing stages of CAD or CAM based production processes. CAD CAM CAD CAM
The development of CAD and CAM and particularly the linkage between the two overcame traditional NC shortcomings in expense, ease of use, and speed by enabling the design and manufacture of a part to be undertaken using the same system of encoding geometrical data. This innovation greatly shortened the period between design and manufacture and greatly expanded the scope of production processes for which automated machinery could be economically used. Just as important, CAD/CAM gave the designer much more direct control over the production process, creating the possibility of completely integrated design and manufacturing processes.
The rapid growth in the use of CAD or CAM technologies after the early 1970s was made possible by the development of mass-produced silicon chips and the microprocessor, resulting in more readily affordable computers. As the price of computers continued to decline and their processing power improved, the use of CAD/CAM broadened from large firms using large-scale mass production techniques to firms of all sizes. The scope of operations to which CAD/CAM was applied broadened as well. In addition to parts-shaping by traditional machine tool processes such as stamping, drilling, milling, and grinding, CAD/CAM has come to be used by firms involved in producing consumer electronics, electronic components, molded plastics, and a host of other products. Computers are also used to control a number of manufacturing processes (such as chemical processing) that are not strictly defined as CAM because the control data are not based on geometrical parameters.
Using CAD, it is possible to simulate in three dimensions the movement of a part through a production process. This process can simulate feed rates, angles and speeds of machine tools, the position of part-holding clamps, as well as range and other constraints limiting the operations of a machine. The continuing development of the simulation of various manufacturing processes is one of the key means by which CAD and CAM systems are becoming increasingly integrated. CAD/CAM systems also facilitate communication among those involved in design, manufacturing, and other processes. This is of particular importance when one firm contracts another to either design or produce a component.
Discrete mathematics discrete math is the study of mathematical structures that are countable or otherwise distinct and separable. Examples of structures that are discrete are combinations, graphs, and logical statements. Discrete structures can be finite or infinite. Discrete mathematics is in contrast to continuous mathematics, which deals with structures which can range in value over the real numbers, or have some non-separable quality.
Number theory is the study of the set of positive whole numbers. 1,2,3,4,5,6,7,..., which are often called the set of natural numbers. We will especially want to study the relationships between different sorts of numbers. Since ancient times, people have separated the natural numbers into a variety of different types.
Spectral graph theory studies connections between combinatorial properties of graphs and the eigenvalues of matrices associated to the graph, such as the adjacency matrix and the Laplacian matrix.
Spectral graph theory has applications to the design and analysis of approximation algorithms for graph partitioning problems, to the study of random walks in graph, and to the construction of expander graphs. It also reveals connections between the above topics, and provides, for example, a way to use random walks to approximately solve graph partitioning problems.
Game theory is a theoretical framework for conceiving social situations among competing players. In some respects, game theory is the science of strategy, or at least the optimal decision-making of independent and competing actors in a strategic setting. The key pioneers of game theory were mathematician John von Neumann and economist Oskar Morgenstern in the 1940s. Mathematician John Nash is regarded by many as providing the first significant extension of the von Neumann and Morgenstern work. The focus of game theory is the game, which serves as a model of an interactive situation among rational players. The key to game theory is that one player's payoff is contingent on the strategy implemented by the other player. The game identifies the players' identities, preferences, and available strategies and how these strategies affect the outcome. Depending on the model, various other requirements or assumptions may be necessary.
Game theory has a wide range of applications, including psychology, evolutionary biology, war, politics, economics, and business. Despite its many advances, game theory is still a young and developing science.
Although there are many types (e.g., symmetric/asymmetric, simultaneous/sequential, et al.) of game theories, cooperative and non-cooperative game theories are the most common. Cooperative game theory deals with how coalitions, or cooperative groups, interact when only the payoffs are known. It is a game between coalitions of players rather than between individuals, and it questions how groups form and how they allocate the payoff among players.
Non-cooperative game theory deals with how rational economic agents deal with each other to achieve their own goals. The most common non-cooperative game is the strategic game, in which only the available strategies and the outcomes that result from a combination of choices are listed. A simplistic example of a real-world non-cooperative game is Rock-Paper-Scissors.
Multimedia is media that uses multiple forms of information content and information processing (e.g. text, audio, graphics,
animation, video, interaction) to inform or entertain the (user) audience. The multimedia theme has two areas, Perceptual Multimedia
Quality (PMQ) and Semantic Content Modeling (SCM). Work on SCM resulted in the creation of the Content Oriented Semantic Modeling Overlay Scheme and associated extensions. PMQ allows the continued work and proliferation of multimedia applications. Drs Stephen Gulliver and George Guinea were the first to use eye tracking for examining multimedia video sequences, and they found methods for distributed multimedia quality from user perspective. The visualization is related with spatial-semantic visualization and virtual reality and moreover their applications in program visualization, text documents, and visualization of medical data. An important point of investigation within the visualization concerns the study of the structure of scientific
knowledge domains and human performance with spatialsemantic visualization and virtual reality. Within the visualization theme, research focuses on visualization of medical programs, data and human factors related with virtual reality display
APPROACH FOR MULTIMEDIA INFORMATION VISUALIZATION The multi-faceted nature of multimedia documents has led to avariety of visual representations for navigating, analyzing andunderstanding of multimedia data sets. As eachrepresentation is specifically designed to address differentaspects of the data, innovative approaches combining severalvisualizations in a single coordinated interface had to beintroduced
Memetic computing algorithms have been successfully applied to a multitude of real-world problems. Although many people employ techniques closely related to memetic algorithms, alternative names such as hybrid genetic algorithms are also employed. Furthermore, many people term their memetic techniques as genetic algorithms.[citation needed]
Researchers have used memetic algorithms to tackle many classical NP problems. To cite some of them: graph partitioning, multidimensional knapsack, travelling salesman problem, quadratic assignment problem, set cover problem, minimal graph coloring, max independent set problem, bin packing problem, and generalized assignment problem.
More recent applications include (but are not limited to) business analytics and data science [3], training of artificial neural networks,[18] pattern recognition,[19] robotic motion planning,[20] beam orientation,[21] circuit design,[22] electric service restoration,[23] medical expert systems,[24] single machine scheduling,[25] automatic timetabling (notably, the timetable for the NHL),[26] manpower scheduling,[27] nurse rostering optimisation,[28] processor allocation,[29] maintenance scheduling (for example, of an electric distribution network),[30] multidimensional knapsack problem,[31] VLSI design,[32] clustering of gene expression profiles,[33] feature/gene selection,[34][35] and multi-class, multi-objective feature selection
Process mining is the missing link between model-based process analysis and dataoriented analysis techniques. Through concrete data sets and easy to use software the course
provides data science knowledge that can be applied directly to analyze and improve processes in a variety of domains. Data science is the profession of the future, because organizations that are unable to use (big) data in a smart way will not survive. It is not sufficient to focus on data
storage and data analysis. The data scientist also needs to relate data to process analysis. Process mining bridges the gap between traditional model-based process analysis (e.g., simulation and
other business process management techniques) and data-centric analysis techniques such as
machine learning and data mining. Process mining seeks the confrontation between event data
(i.e., observed behavior) and process models (hand-made or discovered automatically). This
technology has become available only recently, but it can be applied to any type of operational
processes (organizations and systems). Example applications include: analyzing treatment
processes in hospitals, improving customer service processes in a multinational, understanding
the browsing behavior of customers using booking site, analyzing failures of a baggage handling
system, and improving the user interface of an X-ray machine. All of these applications have in
common that dynamic behavior needs to be related to process models. Hence, we refer to this as
"data science in action". The course explains the key analysis techniques in process mining.
Participants will learn various process discovery algorithms. These can be used to automatically
learn process models from raw event data. Various other process analysis techniques that use
event data will be presented. Moreover, the course will provide easy-to-use software, real-life
data sets, and practical skills to directly apply the theory in a variety of application domains. This
course starts with an overview of approaches and technologies that use event data to support
decision making and business process (re)design. Then the course focuses on process mining as a
bridge between data mining and business process modeling. The course is at an introductory
level with various practical assignments. The course covers the three main types of process
mining. 1. The first type of process mining is discovery. A discovery technique takes an event
log and produces a process model without using any a-priori information. An example is the
Alpha-algorithm that takes an event log and produces a process model (a Petri net) explaining
the behavior recorded in the log. 2. The second type of process mining is conformance. Here, an
existing process model is compared with an event log of the same process. Conformance
checking can be used to check if reality, as recorded in the log, conforms to the model and vice
versa. 3. The third type of process mining is enhancement. Here, the idea is to extend or improve
an existing process model using information about the actual process recorded in some event log.
Whereas conformance checking measures the alignment between model and reality, this third
type of process mining aims at changing or extending the a-priori model. An example is the
extension of a process model with performance information, e.g., showing bottlenecks. Process
mining techniques can be used in an offline, but also online setting. The latter is known as
operational support. An example is the detection of non-conformance at the moment the 
deviation actually takes place. Another example is time prediction for running cases, i.e., given a
partially executed case the remaining processing time is estimated based on historic information
of similar cases. Process mining provides not only a bridge between data mining and business
process management; it also helps to address the classical divide between "business" and "IT".
Evidence-based business process management based on process mining helps to create a
common ground for business process improvement and information systems development. The
course uses many examples using real-life event logs to illustrate the concepts and algorithms.
After taking this course, one is able to run process mining projects and have a good
understanding of the Business Process Intelligence field. After taking this course you should: -
have a good understanding of Business Process Intelligence techniques (in particular process
mining), - understand the role of Big Data in today’s society, - be able to relate process mining
techniques to other analysis techniques such as simulation, business intelligence, data mining,
machine learning, artificial intelligence, and verification, - be able to apply basic process discovery techniques to learn
a process model from an event log (both manually and using tools), - be able to apply basic
conformance checking techniques to compare event logs and process models (both manually and
using tools), - be able to extend a process model with information extracted from the event log
(e.g., show bottlenecks), - have a good understanding of the data needed to start a process mining
project, - be able to characterize the questions that can be answered based on such event data, -
explain how process mining can also be used for operational support (prediction and
recommendation), and - be able to conduct process mining projects in a structured manner.
This course is aimed at both students and professionals. A basic understanding of logic,
sets, and statistics (at the undergraduate level) is assumed. Basic computer skills are required to
use the software provided with the course (but no programming experience is needed).
Participants are also expected to have an interest in process modeling and data mining but no
specific prior knowledge is assumed as these concepts are introduced in the course.
machine learning artificial intelligence algorithms 
Introduction and Data Mining
This first module contains general course information (syllabus, grading information) as
well as the first lectures introducing data mining and process mining.
Process Models and Process Discovery
In this module we introduce process models and the key feature of process mining:
discovering process models from event data.
Different Types of Process Models
Now that you know the basics of process mining, it is time to dive a little bit deeper and
show you other ways of discovering a process model from event data.
Process Discovery Techniques and Conformance Checking
In this module we conclude process discovery by discussing alternative approaches. We
also introduce how to check the conformance of the event data and the process model.
Enrichment of Process Models
In this module we focus on enriching process models. We can for instance add the data
aspect to process models, show bottlenecks on the process model and analyse the social aspects
of the process.
Operational Support and Conclusion
In this final module we discuss how process mining can be applied on running processes.
We also address how to get the (right) event data, process mining software, and how to get from
data to results. Companies have realized they need to hire data scientists, academic institutions are scrambling to put together datascience programs, and publications are touting data science as a hot—even ‘‘sexy’’—career choice. However, there is
confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses
into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly
what is data science. One reason is that data science is intricately intertwined with other important concepts also of
growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to
associate what a practitioner does with the definition of the practitioner’s field; this can result in overlooking the
fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost
importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve
business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to
begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better
understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be
comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close
by offering, as examples, a partial list of fundamental principles underlying data science.
Introduction
With vast amounts of data now available, companies in
almost every industry are focused on exploiting data for
competitive advantage. The volume and variety of data have
far outstripped the capacity of manual analysis, and in some
cases have exceeded the capacity of conventional databases.
At the same time, computers have become far more powerful,
networking is ubiquitous, and algorithms have been developed that can connect datasets to enable broader and deeper
analyses than previously possible. The convergence of these
phenomena has given rise to the increasingly widespread
business application of data science.
Companies across industries have realized that they need to
hire more data scientists. Academic institutions are scrambling to put together programs to train data scientists. Publications are touting data science as a hot career choice and
even ‘‘sexy.’’1 However, there is confusion about what exactly
is data science, and this confusion could well lead to
ª Foster Provost and Tom Fawcett 2013; Published by Mary Ann Liebert, Inc. This article is available under the Creative Commons License CC-BY-NC (http://creativecommons.org/
licenses/by-nc/4.0). This license permits non-commercial use, distribution and reproduction in any medium, provided the original work is properly cited. Permission only needs to be
obtained for commercial use and can be done via RightsLink.
DATA SCIENCE
AND ITS
RELATIONSHIP
TO BIG DATA AND
DATA-DRIVEN
DECISION MAKING
Foster Provost1 and Tom Fawcett2
disillusionment as the concept diffuses into meaningless buzz.
In this article, we argue that there are good reasons why it has
been hard to pin down what exactly is data science. One
reason is that data science is intricately intertwined with other
important concepts, like big data and data-driven decision
making, which are also growing in importance and attention.
Another reason is the natural tendency, in the absence of
academic programs to teach one otherwise, to associate what
a practitioner actually does with the definition of the practitioner’s field; this can result in overlooking the fundamentals of the field.
At the moment, trying to define the boundaries of data science precisely is not of foremost importance. Data-science
academic programs are being developed, and in an academic
setting we can debate its boundaries. However, in order for
data science to serve business effectively, it is important (i) to
understand its relationships to these other important and
closely related concepts, and (ii) to
begin to understand what are the
fundamental principles underlying
data science. Once we embrace
(ii), we can much better understand and explain exactly what
data science has to offer. Furthermore, only once we embrace (ii)
should we be comfortable calling it data science.
In this article, we present a perspective that addresses all these
concepts. We first work to disentangle this set of closely interrelated concepts. In the process, we highlight data science
as the connective tissue between data-processing technologies
(including those for ‘‘big data’’) and data-driven decision
making. We discuss the complicated issue of data science as a
field versus data science as a profession. Finally, we offer as
examples a list of some fundamental principles underlying
data science.
Data Science
At a high level, data science is a set of fundamental principles
that support and guide the principled extraction of information and knowledge from data. Possibly the most closely
related concept to data science is data mining—the actual
extraction of knowledge from data via technologies that incorporate these principles. There are hundreds of different
data-mining algorithms, and a great deal of detail to the
methods of the field. We argue that underlying all these many
details is a much smaller and more concise set of fundamental principles.
These principles and techniques are applied broadly across
functional areas in business. Probably the broadest business
applications are in marketing for tasks such as targeted
marketing, online advertising, and recommendations for
cross-selling. Data science also is applied for general customer
relationship management to analyze customer behavior in
order to manage attrition and maximize expected customer
value. The finance industry uses data science for credit scoring
and trading and in operations via fraud detection and workforce management. Major retailers from Wal-Mart to Amazon
apply data science throughout their businesses, from marketing to supply-chain management. Many firms have differentiated themselves strategically with data science, sometimes
to the point of evolving into data-mining companies.
But data science involves much more than just data-mining
algorithms. Successful data scientists must be able to view
business problems from a data perspective. There is a fundamental structure to data-analytic thinking, and basic
principles that should be understood. Data science draws
from many ‘‘traditional’’ fields of study. Fundamental principles of causal analysis must be understood. A large portion
of what has traditionally been studied within the field of
statistics is fundamental to data
science. Methods and methology
for visualizing data are vital. There
are also particular areas where
intuition, creativity, common
sense, and knowledge of a particular application must be brought
to bear. A data-science perspective
provides practitioners with structure and principles, which
give the data scientist a framework to systematically treat
problems of extracting useful knowledge from data.
Data Science in Action
For concreteness, let’s look at two brief case studies of analyzing data to extract predictive patterns. These studies illustrate different sorts of applications of data science. The
first was reported in the New York Times:
Hurricane Frances was on its way, barreling across
the Caribbean, threatening a direct hit on Florida’s
Atlantic coast. Residents made for higher ground,
but far away, in Bentonville, Ark., executives at WalMart Stores decided that the situation offered a great
opportunity for one of their newest data-driven
weapons.predictive technology.
A week ahead of the storm’s landfall, Linda M.
Dillman, Wal-Mart’s chief information officer,
pressed her staff to come up with forecasts based on
what had happened when Hurricane Charley struck
several weeks earlier. Backed by the trillions of bytes’
worth of shopper history that is stored in WalMart’s data warehouse, she felt that the company
could ‘‘start predicting what’s going to happen, instead of waiting for it to happen,’’ as she put it.2
Consider why data-driven prediction might be useful in this
scenario. It might be useful to predict that people in the path
‘‘PUBLICATIONS ARE TOUTING
DATA SCIENCE AS A HOT CAREER
CHOICE AND EVEN ‘SEXY.’’’
DATA SCIENCE AND BIG DATA
Provost and Fawcett
52BD BIG DATA MARCH 2013
of the hurricane would buy more bottled water. Maybe, but it
seems a bit obvious, and why do we need data science to
discover this? It might be useful to project the amount of
increase in sales due to the hurricane, to ensure that local
Wal-Marts are properly stocked. Perhaps mining the data
could reveal that a particular DVD sold out in the hurricane’s
path—but maybe it sold out that week at Wal-Marts across
the country, not just where the hurricane landing was imminent. The prediction could be somewhat useful, but
probably more general than Ms. Dillman was intending.
It would be more valuable to discover patterns due to the
hurricane that were not obvious. To do this, analysts might
examine the huge volume of Wal-Mart data from prior,
similar situations (such as Hurricane Charley earlier in the
same season) to identify unusual local demand for products.
From such patterns, the company might be able to anticipate
unusual demand for products and
rush stock to the stores ahead of
the hurricane’s landfall.
Indeed, that is what happened.
The New York Times reported
that: ‘‘. the experts mined the
data and found that the stores
would indeed need certain products—and not just the usual
flashlights. ‘We didn’t know in the
past that strawberry Pop-Tarts
increase in sales, like seven times their normal sales rate,
ahead of a hurricane,’ Ms. Dillman said in a recent interview.’
And the pre-hurricane top-selling item was beer.*’’’2
Consider a second, more typical business scenario and how it
might be treated from a data perspective. Assume you just
landed a great analytical job with MegaTelCo, one of the
largest telecommunication firms in the United States. They
are having a major problem with customer retention in their
wireless business. In the mid-Atlantic region, 20% of cellphone customers leave when their contracts expire, and it is
getting increasingly difficult to acquire new customers. Since
the cell-phone market is now saturated, the huge growth in
the wireless market has tapered off. Communications companies are now engaged in battles to attract each other’s
customers while retaining their own. Customers switching
from one company to another is called churn, and it is expensive all around: one company must spend on incentives to
attract a customer while another company loses revenue
when the customer departs.
You have been called in to help understand the problem and
to devise a solution. Attracting new customers is much more
expensive than retaining existing ones, so a good deal of
marketing budget is allocated to prevent churn. Marketing
has already designed a special retention offer. Your task is to
devise a precise, step-by-step plan for how the data science
team should use MegaTelCo’s vast data resources to decide
which customers should be offered the special retention deal
prior to the expiration of their contracts. Specifically, how
should MegaTelCo decide on the set of customers to target to
best reduce churn for a particular incentive budget? Answering this question is much more complicated than it
seems initially.
Data Science and Data-Driven
Decision Making
Data science involves principles, processes, and techniques
for understanding phenomena via the (automated) analysis
of data. For the perspective of this article, the ultimate goal of
data science is improving decision making, as this generally is
of paramount interest to business. Figure 1 places data science
in the context of other closely
related and data-related processes in the organization. Let’s
start at the top.
Data-driven decision making
(DDD)3 refers to the practice of
basing decisions on the analysis
of data rather than purely on intuition. For example, a
marketer could select advertisements based purely on her
long experience in the field and her eye for what will work.
Or, she could base her selection on the analysis of data regarding how consumers react to different ads. She could also
use a combination of these approaches. DDD is not an all-ornothing practice, and different firms engage in DDD to
greater or lesser degrees.
The benefits of data-driven decision making have been demonstrated conclusively. Economist Erik Brynjolfsson and his
colleagues from MIT and Penn’s Wharton School recently
conducted a study of how DDD affects firm performance.3
They developed a measure of DDD that rates firms as to how
strongly they use data to make decisions across the company.
They show statistically that the more data-driven a firm is, the
more productive it is—even controlling for a wide range of
possible confounding factors. And the differences are not small:
one standard deviation higher on the DDD scale is associated
with a 4–6% increase in productivity. DDD also is correlated
with higher return on assets, return on equity, asset utilization,
and market value, and the relationship seems to be causal.
Our two example case studies illustrate two different sorts of
decisions: (1) decisions for which ‘‘discoveries’’ need to be
‘‘FROM SUCH PATTERNS, THE
COMPANY MIGHT BE ABLE TO
ANTICIPATE UNUSUAL DEMAND
FOR PRODUCTS AND RUSH STOCK
TO THE STORES AHEAD OF THE
HURRICANE’S LANDFALL.’’
*Of course! What goes better with strawberry Pop-Tarts than a nice cold beer?
Provost and Fawcett
ORIGINAL ARTICLE
MARY ANN LIEBERT, INC. VOL. 1 NO. 1 MARCH 2013 BIG DATA BD53
made within data, and (2) decisions that repeat, especially at
massive scale, and so decision making can benefit from even
small increases in accuracy based on data analysis. The WalMart example above illustrates a type-1 problem. Linda
Dillman would like to discover knowledge that will help WalMart prepare for Hurricane Frances’s imminent arrival. Our
churn example illustrates a type2 DDD problem. A large telecommunications company may
have hundreds of millions of
customers, each a candidate for
defection. Tens of millions of
customers have contracts expiring each month, so each one of
them has an increased likelihood of defection in the near
future. If we can improve our ability to estimate, for a given
customer, how profitable it would be for us to focus on her,
we can potentially reap large benefits by applying this ability
to the millions of customers in the population. This same
logic applies to many of the areas where we have seen the
most intense application of data science and data mining:
direct marketing, online advertising, credit scoring, financial
trading, help-desk management, fraud detection, search
ranking, product recommendation, and so on.
The diagram in Figure 1 shows data science supporting datadriven decision making, but also overlapping with it. This
highlights the fact that, increasingly, business decisions are
being made automatically by computer systems. Different
industries have adopted automatic decision making at different rates. The finance and telecommunications industries
were early adopters. In the 1990s, automated decision making
changed the banking and consumer-credit industries dramatically. In the 1990s, banks and telecommunications
companies also implemented massive-scale systems for
managing data-driven fraud control decisions. As retail systems
were increasingly computerized,
merchandising decisions were
automated. Famous examples include Harrah’s casinos’ reward
programs and the automated
recommendations of Amazon and
Netflix. Currently we are seeing a revolution in advertising,
due in large part to a huge increase in the amount of time
consumers are spending online and the ability online to make
(literally) split-second advertising decisions.
Data Processing and ‘‘Big Data’’
Despite the impression one might get from the media, there is
a lot to data processing that is not data science. Data engineering and processing are critical to support data-science
activities, as shown in Figure 1, but they are more general and
are useful for much more. Data-processing technologies are
important for many business tasks that do not involve extracting knowledge or data-driven decision making, such as
efficient transaction processing, modern web system processing, online advertising campaign management, and others.
‘‘Big data’’ technologies, such as Hadoop, Hbase, CouchDB,
and others have received considerable media attention recently. For this article, we will simply take big data to mean
datasets that are too large for traditional data-processing
systems and that therefore require new technologies. As with
the traditional technologies, big data technologies are used
for many tasks, including data engineering. Occasionally, big
data technologies are actually used for implementing datamining techniques, but more often the well-known big data
technologies are used for data processing in support of the
data-mining techniques and other data-science activities, as
represented in Figure 1.
Economist Prasanna Tambe of New York University’s Stern
School has examined the extent to which the utilization of big
data technologies seems to help firms.4 He finds that, after
controlling for various possible confounding factors, the use
of big data technologies correlates with significant additional
productivity growth. Specifically, one standard deviation higher
utilization of big data technologies is associated with 1–3%
higher productivity than the average firm; one standard deviation lower in terms of big data utilization is associated with
1–3% lower productivity. This leads to potentially very large
productivity differences between the firms at the extremes.
FIG. 1. Data science in the context of closely related processes
in the organization.
‘‘THE BENEFITS OF DATA-DRIVEN
DECISION MAKING HAVE BEEN
DEMONSTRATED CONCLUSIVELY.’’
DATA SCIENCE AND BIG DATA
Provost and Fawcett
54BD BIG DATA MARCH 2013
From Big Data 1.0 to Big Data 2.0
One way to think about the state of big data technologies is to
draw an analogy with the business adoption of internet
technologies. In Web 1.0, businesses busied themselves with
getting the basic internet technologies in place so that they
could establish a web presence, build electronic commerce
capability, and improve operating efficiency. We can think of
ourselves as being in the era of Big Data 1.0, with firms
engaged in building capabilities to process large data. These
primarily support their current operations—for example, to
make themselves more efficient.
With Web 1.0, once firms had
incorporated basic technologies
thoroughly (and in the process had
driven down prices) they started to
look further. They began to ask
what the web could do for them,
and how it could improve upon
what they’d always done. This
ushered in the era of Web 2.0, in
which new systems and companies
started to exploit the interactive nature of the web. The
changes brought on by this shift in thinking are extensive and
pervasive; the most obvious are the incorporation of socialnetworking components and the rise of the ‘‘voice’’ of the
individual consumer (and citizen).
Similarly, we should expect a Big Data 2.0 phase to follow Big
Data 1.0. Once firms have become capable of processing
massive data in a flexible fashion, they should begin asking:
What can I now do that I couldn’t do before, or do better than I
could do before? This is likely to usher in the golden era of data
science. The principles and techniques of data science will be
applied far more broadly and far more deeply than they are
today.
It is important to note that in the Web-1.0 era, some precocious companies began applying Web-2.0 ideas far ahead
of the mainstream. Amazon is a prime example, incorporating the consumer’s ‘‘voice’’ early on in the rating of
products and product reviews (and deeper, in the rating of
reviewers). Similarly, we see some companies already applying Big Data 2.0. Amazon again is a company at the
forefront, providing data-driven recommendations from
massive data. There are other examples as well. Online advertisers must process extremely large volumes of data (billions of ad impressions per day is not unusual) and maintain
a very high throughput (real-time bidding systems make
decisions in tens of milliseconds). We should look to these
and similar industries for signs of advances in big data and
data science that subsequently will be adopted by other
industries.
Data-Analytic Thinking
One of the most critical aspects of data science is the support
of data-analytic thinking. Skill at thinking data-analytically is
important not just for the data scientist but throughout the
organization. For example, managers and line employees in
other functional areas will only get the best from the company’s data-science resources if they have some basic understanding of the fundamental principles. Managers in
enterprises without substantial data-science resources should
still understand basic principles in order to engage consultants on an informed basis. Investors in data-science ventures
need to understand the fundamental principles in order to assess investment opportunities
accurately. More generally, businesses increasingly are driven by
data analytics, and there is great
professional advantage in being
able to interact competently
with and within such businesses.
Understanding the fundamental
concepts, and having frameworks
for organizing data-analytic thinking, not only will allow one
to interact competently, but will help to envision opportunities for improving data-driven decision making or to see
data-oriented competitive threats.
Firms in many traditional industries are exploiting new and
existing data resources for competitive advantage. They employ data-science teams to bring advanced technologies to
bear to increase revenue and to decrease costs. In addition,
many new companies are being developed with data mining
as a key strategic component. Facebook and Twitter, along
with many other ‘‘Digital 100’’ companies,5 have high valuations due primarily to data assets they are committed to
capturing or creating.{ Increasingly, managers need to manage data-analytics teams and data-analysis projects, marketers
have to organize and understand data-driven campaigns,
venture capitalists must be able to invest wisely in businesses
with substantial data assets, and business strategists must be
able to devise plans that exploit data.
As a few examples, if a consultant presents a proposal to
exploit a data asset to improve your business, you should be
able to assess whether the proposal makes sense. If a competitor announces a new data partnership, you should recognize when it may put you at a strategic disadvantage. Or,
let’s say you take a position with a venture firm and your first
project is to assess the potential for investing in an advertising
company. The founders present a convincing argument that
they will realize significant value from a unique body of data
they will collect, and on that basis, are arguing for a substantially higher valuation. Is this reasonable? With an
‘‘SIMILARLY, WE SHOULD
EXPECT A BIG DATA 2.0 PHASE
TO FOLLOW BIG DATA 1.0 . THIS
IS LIKELY TO USHER IN THE
GOLDEN ERA OF DATA SCIENCE.’’
{
Of course, this is not a new phenomenon. Amazon and Google are well-established companies that obtain tremendous value from their data assets.
Provost and Fawcett
ORIGINAL ARTICLE
MARY ANN LIEBERT, INC. VOL. 1 NO. 1 MARCH 2013 BIG DATA BD55
understanding of the fundamentals of data science, you
should be able to devise a few probing questions to determine
whether their valuation arguments are plausible.
On a scale less grand, but probably more common, dataanalytics projects reach into all business units. Employees
throughout these units must interact with the data-science
team. If these employees do not have a fundamental
grounding in the principles of data-analytic thinking, they
will not really understand what is happening in the business.
This lack of understanding is much more damaging in datascience projects than in other technical projects, because the
data science supports improved
decision making. Data-science
projects require close interaction
between the scientists and the
business people responsible for the
decision making. Firms in which
the business people do not understand what the data scientists are
doing are at a substantial disadvantage, because they waste time
and effort or, worse, because they
ultimately make wrong decisions.
A recent article in Harvard Business Review concludes: ‘‘For all the breathless promises about
the return on investment in Big Data, however, companies
face a challenge. Investments in analytics can be useless, even
harmful, unless employees can incorporate that data into
complex decision making.’’6
Some Fundamental Concepts
of Data Science
There is a set of well-studied, fundamental concepts underlying the principled extraction of knowledge from data, with
both theoretical and empirical backing. These fundamental
concepts of data science are drawn from many fields that
study data analytics. Some reflect the relationship between
data science and the business problems to be solved. Some
reflect the sorts of knowledge discoveries that can be made
and are the basis for technical solutions. Others are cautionary and prescriptive. We briefly discuss a few here. This
list is not intended to be exhaustive; detailed discussions even
of the handful below would fill a book.* The important thing
is that we understand these fundamental concepts.
Fundamental concept: Extracting useful knowledge from data
to solve business problems can be treated systematically by following a process with reasonably well-defined stages. The CrossIndustry Standard Process for Data Mining7 (CRISP-DM) is
one codification of this process. Keeping such a process in
mind can structure our thinking about data analytics problems. For example, in actual practice one repeatedly sees analytical ‘‘solutions’’ that are not based on careful analysis of
the problem or are not carefully evaluated. Structured thinking about analytics emphasizes these often underappreciated
aspects of supporting decision making with data. Such
structured thinking also contrasts critical points at which
human intuition and creativity is necessary versus points at
which high-powered analytical tools can be brought to bear.
Fundamental concept: Evaluating data-science results requires
careful consideration of the context in which they will be used.
Whether knowledge extracted from data will aid in decision
making depends critically on the
application in question. For our
churn-management example, how
exactly are we going to use the
patterns that are extracted from
historical data? More generally,
does the pattern lead to better decisions than some reasonable alternative? How well would one have
done by chance? How well would
one do with a smart ‘‘default’’ alternative? Many data science evaluation frameworks are based on
this fundamental concept.
Fundamental concept: The relationship between the business
problem and the analytics solution often can be decomposed into
tractable subproblems via the framework of analyzing expected
value. Various tools for mining data exist, but business
problems rarely come neatly prepared for their application.
Breaking the business problem up into components corresponding to estimating probabilities and computing or estimating values, along with a structure for recombining the
components, is broadly useful. We have many specific tools
for estimating probabilities and values from data. For our
churn example, should the value of the customer be taken
into account in addition to the likelihood of leaving? It is
difficult to realistically assess any customer-targeting solution
without phrasing the problem as one of expected value.
Fundamental concept: Information technology can be used to
find informative data items from within a large body of data.
One of the first data-science concepts encountered in business-analytics scenarios is the notion of finding correlations.
‘‘Correlation’’ often is used loosely to mean data items that
provide information about other data items—specifically,
known quantities that reduce our uncertainty about unknown quantities. In our churn example, a quantity of interest is the likelihood that a particular customer will leave
after her contract expires. Before the contract expires, this
would be an unknown quantity. However, there may be
known data items (usage, service history, how many friends
‘‘FACEBOOK AND TWITTER,
ALONG WITH MANY OTHER
‘DIGITAL 100’ COMPANIES, HAVE
HIGH VALUATIONS DUE
PRIMARILY TO DATA ASSETS
THEY ARE COMMITTED TO
CAPTURING OR CREATING.’’
*And they do; see http://data-science-for-biz.com.
DATA SCIENCE AND BIG DATA
Provost and Fawcett
56BD BIG DATA MARCH 2013
have canceled contracts) that correlate with our quantity of
interest. This fundamental concept underlies a vast number
of techniques for statistical analysis, predictive modeling, and
other data mining.
Fundamental concept: Entities that are similar with respect to
known features or attributes often are similar with respect to
unknown features or attributes. Computing similarity is one of
the main tools of data science. There are many ways to
compute similarity and more are invented each year.
Fundamental concept: If you look
too hard at a set of data, you will
find something—but it might not
generalize beyond the data you’re
observing. This is referred to as
‘‘overfitting’’ a dataset. Techniques
for mining data can be very powerful, and the need to detect and
avoid overfitting is one of the most important concepts to
grasp when applying data-mining tools to real problems. The
concept of overfitting and its avoidance permeates data science processes, algorithms, and evaluation methods.
Fundamental concept: To draw causal conclusions, one must
pay very close attention to the presence of confounding factors,
possibly unseen ones. Often, it is not enough simply to uncover correlations in data; we may want to use our models to
guide decisions on how to influence the behavior producing
the data. For our churn problem, we want to intervene and
cause customer retention. All methods for drawing causal
conclusions—from interpreting the coefficients of regression
models to randomized controlled experiments—incorporate
assumptions regarding the presence or absence of confounding factors. In applying such methods, it is important
to understand their assumptions clearly in order to understand the scope of any causal claims.
Chemistry Is Not About Test Tubes: Data
Science vs. the Work of the Data Scientist
Two additional, related complications combine to make it
more difficult to reach a common understanding of just
what is data science and how it fits with other related
concepts.
First is the dearth of academic programs focusing on data
science. Without academic programs defining the field for
us, we need to define the field for ourselves. However, each
of us sees the field from a different perspective and thereby
forms a different conception. The dearth of academic programs is largely due to the inertia associated with academia
and the concomitant effort involved in creating new academic programs—especially ones that span traditional disciplines. Universities clearly see the need for such programs,
and it is only a matter of time before this first complication
will be resolved. For example, in New York City alone, two
top universities are creating degree programs in data science. Columbia University is in the process of creating a
master’s degree program within its new Institute for Data
Sciences and Engineering (and has founded a center focusing on the foundations of data science), and NYU will
commence a master’s degree program in data science in
fall 2013.
The second complication builds
on confusion caused by the first.
Workers tend to associate with
their field the tasks they spend
considerable time on or those
they find challenging or rewarding. This is in contrast to the tasks
that differentiate the field from
other fields. Forsythe described this phenomenon in an
ethnographic study of practitioners in artificial intelligence
(AI):
The AI specialists I describe view their professional
work as science (and in some cases engineering).The scientists’ work and the approach they
take to it make sense in relation to a particular view
of the world that is taken for granted in the
laboratory.Wondering what it means to ‘‘do AI,’’
I have asked many practitioners to describe their own
work. Their answers invariably focus on one or more
of the following: problem solving, writing code, and
building systems.8
Forsythe goes on to explain that the AI practitioners focus on
these three activities even when it is clear that they spend
much time doing other things (even less related specifically to
AI). Importantly, none of these three tasks differentiates AI
from other scientific and engineering fields. Clearly just being
very good at these three things does not an AI scientist make.
And as Forsythe points out, technically the latter two are not
even necessary, as the lab director, a famous AI Scientist, had
not written code or built systems for years. Nonetheless, these
are the tasks the AI scientists saw as defining their work—
they apparently did not explicitly consider the notion of what
makes doing AI different from doing other tasks that involve
problem solving, writing code, and system building. (This is
possibly due to the fact that in AI, there were academic distinctions to call on.)
Taken together, these two complications cause particular
confusion in data science, because there are few academic
distinctions to fall back on, and moreover, due to the state of
the art in data processing, data scientists tend to spend a
majority of their problem-solving time on data preparation
and processing. The goal of such preparation is either to
subsequently apply data-science methods or to understand
the results. However, that does not change the fact that the
day-to-day work of a data scientist—especially an entry-level
one—may be largely data processing. This is directly analogous to an entry-level chemist spending the majority of her
time doing technical lab work. If this were all she were trained
to do, she likely would not be rightly called a chemist but
rather a lab technician. Important for being a chemist is that
this work is in support of the application of the science of
chemistry, and hopefully the eventual advancement to jobs
involving more chemistry and less technical work. Similarly
for data science: a chief scientist in a data-science-oriented
company will do much less data processing and more dataanalytics design and interpretation.
At the time of this writing, discussions of data science inevitably mention not just the analytical skills but the popular
tools used in such analysis. For example, it is common to see
job advertisements mentioning data-mining techniques
(random forests, support vector machines), specific application areas (recommendation systems, ad placement optimization), alongside popular software tools for processing big
data (SQL, Hadoop, MongoDB). This is natural. The particular concerns of data science in business are fairly new, and
businesses are still working to figure out how best to address
them. Continuing our analogy, the state of data science may
be likened to that of chemistry in the mid-19th century, when
theories and general principles were being formulated and the
field was largely experimental. Every good chemist had to be a
competent lab technician. Similarly, it is hard to imagine a
working data scientist who is not proficient with certain sorts
of software tools. A firm may be well served by requiring that
their data scientists have skills to access, prepare, and process
data using tools the firm has adopted.
Nevertheless, we emphasize that there is an important reason
to focus here on the general principles of data science. In ten
years’ time, the predominant technologies will likely have
changed or advanced enough that today’s choices would seem
quaint. On the other hand, the general principles of data
science are not so differerent than they were 20 years ago and
likely will change little over the coming decades.
Conclusion
Underlying the extensive collection of techniques for mining
data is a much smaller set of fundamental concepts comprising data science. In order for data science to flourish as a
field, rather than to drown in the flood of popular attention,
we must think beyond the algorithms, techniques, and tools
in common use. We must think about the core principles and
concepts that underlie the techniques, and also the systematic
thinking that fosters success in data-driven decision making.
These data science concepts are general and very broadly
applicable.
Success in today’s data-oriented business environment requires being able to think about how these fundamental
concepts apply to particular business problems—to think
data-analytically. This is aided by conceptual frameworks that
themselves are part of data science. For example, the automated extraction of patterns from data is a process with welldefined stages. Understanding this process and its stages helps
structure problem solving, makes it more systematic, and
thus less prone to error.
There is strong evidence that business performance can be
improved substantially via data-driven decision making,3 big
data technologies,4 and data-science techniques based on big
data.9,10 Data science supports data-driven decision making—and sometimes allows making decisions automatically
at massive scale—and depends upon technologies for ‘‘big
data’’ storage and engineering. However, the principles of
data science are its own and should be considered and discussed According to (Minelli et al. 2014), the task of understanding a program corresponds to
more than 50% of all the maintenance activity. The main reason for this is the absence
of abstractions on the applications, generally with thousands of lines of code distributed
in hundreds of files, which makes understanding more difficult.
In cooperation with industry partners we have carried out projects to tackle dif- ferent
modernization challenges: 1) Migration from Oracle Forms to Java and .Net (Garcés et al.
2015; Wikipedia 2016; Garcés et al. 2018); 2) Restructuring of Java Enterprise Edition
(JEE) applications from mono- lithic architectures to microservices (Escobar et al. 2016);
and 3) Maintenance of Ruby on Rails (RoR) applications developed by Agile practitioners
(García and Garcés 2017). Literature (Anquetil and Laval 2011; Mancoridis et al. 1999)
and our experi- ence in these projects have shown us that, besides to the related complexity with the size of the applications, the understanding of the programs is difficult for the
following reasons: i) lack of documentation about the design; ii) lack of knowledge about
architectural decisions taken in the original design because of developers’ turnover; and
iii) the degradation of previously made decisions, including additions and modifications
done over time.
For every project, we reviewed tools that allow building abstractions of higher level, i.e.,
architectural views. For example, we found tools that obtain commodity views such as
UML class (or package) diagrams from Java source code. We found that they are completely dependent on the language/technology, and that the views that they produce are
predefined and do not necessarily correspond to particular understanding needs. Based on
the experience on these projects, and taking inspi- ration from the general process defined
by (Tilley 2009), we present an approach based on Model-Driven Reverse Engineering
(MDRE) (Brunelière et al. 2014; Rugaber and Stirewalt 2004) that allows us to annotate a
Platform Independent Metamodel (PIM) (referred to as Architectural model), and generate a specification of a graphic editor (which, for purposes of the evaluation, is Sirius1
(Mendivelso et al. 2017)). Therefore, one can see in the editor different perspectives of the
application according to the views defined in the specification; e.g., level of coupling
between functional modules.
The contributions of our work are: i) A view generation process extensible to many
source application technologies and views specification frameworks. Our approach can be
applicable to many technologies by plugging new parsers to the workflow. Our proposal is
view specification agnostic because the user specifies the views at architecture level by
using annotations. The annotations are translated to the Technology Independent Views
Specification model (TIVS) first, and then to the particular view specification framework
(e.g., Sirius). ii) Clustering algorithms that group structural elements of the source applications, at an early step of the process. Thus, leveraging the last step which is views render.
iii) Annotations that help users not only to specify the structural elements to be displayed,
but also to represent particular software metrics via the elements style; that is, colour, size,
and labels. For example, if there exists a relationship between two modules A and B,
which is thicker than the relationship between B and C, then it means that the coupling
Mendivelso et al. Journal of Software Engineering Research and Development (2018) 6:16 Page 2 of 23
A-B is higher compared with that of B-C. Annotations refer to measures present in a
Metrics model, which are calculated by a Metrics transformation.
When comparing our approach to the closest related work (http://themoosebook.org/
book/index.html; Bergel et al. 2014), one finds that our approach resembles the competition’s in two aspects: i) both are able to take, as input, artifacts that conform to different
technologies; and ii) views style reflects software metrics. However, it distinguishes itself
from the rest because: i) Whereas related work transforms source artifacts to a unique
pivot, we have a pivot (referred to as Architecture metamodel ) for each family of technologies. When having a unique pivot, there is the risk of finding no direct correspondences
between the source technology and the intermediate representation. As a consequence,
the user who develops the parser ends up establishing correspondences that suit her purposes, but that are not necessarily understandable by others. Therefore, our approximation
aims at having a balance between reusability (among technologies of a same family) and
expressiveness (keeping some semantics of the source tech- nologies). As an illustration,
we have a pivot for 4GL languages that is useful to reverse engineering Oracle Forms,
Visual Basic and Delphi. ii) In related work, the elements (and metrics) displayed in the
view are limited to those explicit in the source structure. In contrast, in our approach, the
user has the possibility of defining new structural elements (and corresponding metrics)
needed for facilitating her software comprehension tasks. For example, in the Oracle
Forms projects, our partners required the notion of functional modules, which is not
present in Forms applications. iii) In related work, the views are rendered in editors under
a fixed technology. In contrast, we have the TIVS model that can be mapped to different
views specification frameworks (i.e., not only Sirius).
To validate our approach and tool, we have applied them in the concrete cases of Oracle Forms, Java EE and Ruby on Rails taking into account the needs that each project
had. We were able to conclude that what was originally done in a specific way for a
particular project could be largely generalized by our approach; thus, improving
flexibility in the tasks of program understanding.
The remaining of the paper is structured as follows: Section 3 presents the moti- vation
of our research based on the experience gained in the three modernization projects mentioned before. Section 4 elaborates on the MDE solution; i.e., the chain of model transformations to generate views for a particular source technology, and how the chain is
parameterized with annotations. Section 5 presents an evaluation. Section 6 extends related
work. Section 7 summarizes conclusions and perspectives for the future.
3 Motivation
3.1 Migration from oracle forms to java and net
Oracle Forms appeared at the end of the 1980s as a programming language and development tool to create Client/Server applications that interact with Oracle databases.
Forms minimizes the need of developers to program common operations like transactions management and coding of CRUD operations. The applications can include PL/
SQL code that allows programmers to enrich the functionality be- yond CRUD. Forms
and Tables are two of the main concepts supporting Forms applications.
The challenges in this project were related to knowing, given a form to be mi- grated,
which are the tables and forms related to it? which functional module does contain it?
Mendivelso et al. Journal of Software Engineering Research and Development (2018) 6:16 Page 3 of 23
and, how is that module related to others? Answering these questions was important to
delimit the migration scope. Similarly, the project was challeng- ing since it lacked:
documentation, availability of those who initially developed the legacy system, directory
hierarchy that organizes the forms, significant naming con- ventions, and relationships
embedded in the PL/SQL code, which is scattered along the elements of the forms. Another challenge is the size of the applications. In this project, the number of forms
ranges from 83 to 178, referenced tables from 101 to 200, blocks from 361 to 765 and
triggers from 2140 to 4406.
When reviewing the state of the art in terms of views for Oracle Forms, we found tools
that showed the graphic interface design, or a navigation tree of the application structural
elements. These tools are Oracle2Java2
], Evo,3 Jheadstart,4 Pitss,5 Ormit.6 Since this was
not enough to meet the challenges, three views were proposed (Garcés et al. 2015): the
Functional modules view where each module group forms. The modules are represented
with circles which size changes according to the amount of forms in it. This view shows
the relationships between the modules that result from synthesizing dependency relationships between forms. Additionally, the view hides the content of each module since it is
the access point to another more detailed view: the Forms view. This view shows the
forms of a selected module. In this view, the size of the elements depends on a metric
which is the number of elements that compose the form. Finally, the Forms and Tables
view shows the different kinds of relationships between Oracle Forms elements: between
forms and tables (e.g., simple and master/detail relations) and between forms (e.g., call
relation). Fig. 6b illustrates the Forms and Tables view.
3.2 Restructuring monolithic JEE applications with microservices
JEE is commonly used in business applications and it proposes a three-layer architecture: presentation, logic, and persistence. The project focused on the logic and
persistence layers, where there are Enterprise Java Beans (EJBs).
The challenges in this project were understanding an existent architecture (in particular,
the coupling level of the logic and persistence EJBs), and proposing a partition of the EJBs
in microservices. For the latter, it was defined as a principle that the highly coupled EJBs
were grouped together and that the loosely coupled ones were isolated and presented as
independent microservices. It was important to provide a list of relations (i.e., invocations)
between the microservices to decide whether or not a set of given EJBs may be split. At
last, it was key to understand how the microservices operate the database tables in execution time to verify that each microservice accesses different tables and, thus, respect the
isolation principle. Satisfying these challenges was important to ease architectural decision
making about how to evolve monolithic applications to microservices. With respect to the
challenge of size, the studied application had 74566 LoC, 624 classes, and 35993 methods.
We found many tools that produce UML diagrams from Java code; for example: Enterprise Architect,7 Modelio,8 IBM Relational,9 StarUML,10 among others. However,
the proposed diagrams are very generic for the migration purpose. We proposed four
views that respond to the particular needs of the project: The EJBs clusters view in
which every cluster groups EJBs of logic and/or persistence. The clusters were represented with circles which dimension is a function of the number of contained EJBs. To
rank the number of lines of code (LOC) of a cluster, we assigned a color to each circle
Mendivelso et al. Journal of Software Engineering Research and Development (2018) 6:16 Page 4 of 23
by using Saffir-Simpson scale (Wikipedia 2017). The Microservices view groups the
clusters of the previous view. The Microservices and tables view shows the diverse types
of relationships that can be established between microservices and database tables:
writing, reading, updating and deletion relations. Finally, there is a view that shows the
dependencies between the logic EJBs that are in the microservices. Fig. 6d illustrates
the EJBs clusters view.
3.3 Maintenance of Ruby on rails applications
Ruby on Rails is an agile development framework for web applications that follows the
architectural pattern Model-View-Controller.
The challenge in this project was knowing which are the Rails models (i.e., per- sistence
entities, services and utilities)? how are these related to one another? and which is the type
of a given relation? This was important in the agile development context to determine the
viability of a new user story and estimate developers’ effort to implemented it. Finally,
Ruby is a dynamically typed language that has differences between its versions. Regarding
the challenge of the application size, in this project, the applications consisted of 15 to 51
Rails models.
Some of the tools we found to face this challenge were: Railroady11 and MySQL Workbench, 12 which offer views that are hard to navigate and lack detailed in- formation about
which are the attributes/methods of a given Rails model. That motivated a new view that
shows all the Rails models (including their attributes and methods) and the relationships
between them (i.e., simple, aggregation and composition). In contrast to the rest of
projects, Ruby on Rails did not require any view customization to reflect metrics. Fig. 6c
illustrates the Rails view.
3.4 Analysis of common features
We have observed the following common needs in the previous projects: It is required to have views that: i) show coarse-grained elements (i.e., containers) that group
fine-grained elements; ii) deploy the detail of every coarse-grained element;
iii) combine coarse and fine-grained elements (i.e., nodes) into one container; iv) show
relationships between elements; v) be navigable from a high to a low level of detail; and
vi) have visual aids (e.g., labels, colors, sizes) to reflect a software metric. Table 1 summaries the views needed in each project and how technology/architec- ture concepts
map to the aforementioned view concepts.
4 Views generation process
4.1 Overview
Even though the projects worked on different technologies, at the end, as we pointed
out in Section 3.4 the architectural visualization needs are similar. Thus, we decided to
study the possibility of generating metric-centered and technology-independent architectural views. We must emphasize that our aim is creating the most common views
observed both in the literature and in our own experiences. Such views serve as a starting point but may require customization to meet all the particular needs. For each new
technology we have to develop some assets (referred to as the Domain Engineering
process) that are then reused for every application that conforms to such a technology
Mendivelso et al. Journal of Software Engineering Research and Development (2018) 6:16 Page 5 of 23
(this is called the Application Engineering process). These terms are originally coined in
(Czarnecki and Eisenecker 2000). Figure 1 (which uses a data flow notation) shows the
two processes. The application engineering level is for the final users (such as architects,
developers, testers, etc. who carry out software comprehension tasks) to generate
metric-centered views for a particular application. The final users provides the application
data to analyze and automatically, the process generates the views of the application. This
process consists of 4 steps (represented as circles): i) data injection to obtain a memory
representation of the input software artifacts; ii) queries on the representation of step 1
that allow us to find the architectural elements of the language/technology; iii) computation of metrics related to such elements; and iv) rendering of architectural elements and
their associated metrics in an editor.
Table 1 Mapping between technology concepts and view concepts
Project View Technology concept View concept
JEE Microservices View Microservice Container
Microservice Relationship Relationship
EJB Clusters View EJB Cluster Container
EJB Cluster Relationship Relationship
Microservices and Tables View Microservice Container
Table Node
Writing Relationship Relationship
Reading Relationship Relationship
Updating Relationship Relationship
Deletion Relationship Relationship
EJB Dependencies Views Class Container
Interface Container
Attribute Node
Method Node
Generalization Relationship
Realization Relationship
Composition Relationship
Dependency Relationship
Ruby on Rails Rails View Package Container
Rails Class Container
Attribute Node
Method Node
Rails Relationship Relationship
Oracle Forms Functional Modules View Module Container
Module Relationship Relationship
Forms View Form Node
Forms and Table Views Form Node
Table Node
Single Table Relationship Relationship
Master-Detail Relationship Relationship
PL/SQL Relationship Relationship
Form Call Relationship Relationship
Mendivelso et al. Journal of Software Engineering Research and Development (2018) 6:16 Page 6 of 23
The domain engineering level is for an MDRE expert who has to provide part of an
infrastructure and reuses other part. Light gray squares represent the assets that the
MDRE expert has to define per each new technology or programming language of
interest. White squares represent assets at the heart of our approach that can be reused
regardless of the source technology or language. Dark gray squares represent the assets
generated per each source application to be analyzed. The assets consist of parsers,
models, metamodels and transformations. Next sections describe these assets in detail.
We use the Oracle Forms project as illustrating example.
4.2 Step 1: Parsers
For step 1, depending on the technology, the MDRE expert has to produce a model
that conforms to the language/technology metamodel from the source data. This task
means to develop or reuse one or more parsers depending on the available input data.
In our experimentation, in the case of Oracle Forms, we developed a single parser in
Java that produces a model conforming to a metamodel of Oracle Forms. For Ruby
code, we implemented two Xtext grammars13 which in turn generated the parsers and
the meta-models for SQL and Rails. For JEE, we used Modisco,14 which gives as a result a model with a full abstract syntax tree of the source code. Such a model conforms
to the standard metamodel so-called KDM.15
machine learning algorithms and data structure artificial intelligence algorithms intelligent agents agent oriented software engineering multi agent system human computer interaction
Besides the source code, we had dynamic data, i.e., logs that saved a trace of the SQL
operations executed. This was useful to complete the model with relationships between
the database tables and the microservices. A specific parser was built for that task.
machine learning and knowledge discovery machine learning artificial intelligence
4.3 Step 2: architecture metamodel and technology2architecture transformation
The MDRE expert has to design an architecture metamodel and a Technol- ogy2Architecture transformation for this step. The architecture metamodel rep- resents the structural
elements that final users want to have in the views. The Technology2Architecture transformation produces an architecture model from the step 1 model.
Fig. 2 shows the architecture metamodel for the Oracle Forms industry case. It provides a way to specify the main concepts needed to represent a typical clien- t/server
4GL application but tries to do so in a more platform independent repre- sentation
(in the MDA sense of the word). This would enable in the future to reuse most of the
metamodel in the domain engineering process of other similar languages (like Delphi
or Visual Basic).
Fig. 1 Process at application/domain engineering level
Mendivelso et al. Journal of Software Engineering Research and Development (2018) 6:16 Page 7 of 23
Some of the main metamodel concepts are: i) ‘Module’ that contains ‘Form’ and
‘Table’; ii) ‘ModuleRelationship’ that represents the relations between the modules; and
iii) A ‘SingleTableRelationship’ that describes a simple relation between a form and a
table. It is worth highlighting that some of these concepts have a direct corre- spondence with the concepts of the technology models (i.e., Form and Table). How- ever,
others are calculated from the information contained in those models by using clustering algorithms in the Technology2Architecture transformation. A clustering algorithm
arranges fine-grained elements in coarse-grained elements by evaluating their attributes
and relationships. Previous works in software comprehension have used clustering
algorithms, see for example (Anquetil and Laval 2011; Mancoridis et al. 1999).
Clustering algorithms were needed in the three industry cases. However, their use
should be assessed on a case-by-case basis because it depends on the final users’ requirements and the software structure. In the next paragraphs, we spell out the requirements
and software restrictions that motivated the use of clustering in the industry cases, as well
as a brief description of the algorithms.
In the Oracle Forms case, the fact of knowing the module containing the form sub- ject
matter of modernization helps developers to delimit the modernization scope. However, it
is not always easy to get the modules because legacy software organi- zation is often quite
poor. To cope with this, we have implemented two clustering algorithms that arrange the
Fig. 2 Architecture metamodel for Oracle Forms
model based coding and model checking
Mendivelso et al. Journal of Software Engineering Research and Development (2018) 6:16 Page 8 of 23
forms, tables and relationships into modules: Menu- based clustering algorithm and Table
betweenness clustering algorithm (Garcés et al. 2015). ‘Module’ is the result of these algorithms that group in a module the forms that they have in common, or that are called
from a same menu of the graphic interface, or that depend on the same tables. In addition,
‘ModuleRelationship’ comes out the calls between forms that are contained in two different modules (the calls are derived from CALL/OPEN queries embedded in the PL/SQL
code).
In the JEE case, developers need suggested options about how to split the legacy applications in microservices. This distribution is not trivial since stakeholders likely face
legacy code with the following problems: 1) lack of design and documentation; 2) absent reasoning on the original design and architecture decisions; 3) undermin- ing of
the original design decisions as many additions and alterations have been made. To
deal with this, we implemented an algorithm that groups together the logic EJBs that
access to the same persistence entities. In addition, we developed another algorithm
that groups clusters in microservices, taking into account the per- centage of EJBs in
common. These algorithms are referred to as EJB Clustering, and Microservice Clustering in previous work (Escobar et al. 2016).
In the Ruby on Rails case, developers need to have a wide vision of the application
before taking (re)design decisions. This vision encompasses diagrams for the logic and
persistence layers of Ruby on Rails applications. This is challenging because, in applications built on top of dynamically typed languages (like Ruby), it is not possible to have all
the information of methods and attributes that will be available in execution; besides that,
the types of the attributes are unknown and thus it is impossible to determine which classes have aggregation/composition relationships. To address this, we have implemented a
clustering algorithm that groups Rails models in packages based on namespaces and
discovered correlation between classes and database tables (García and Garcés 2017).
This helps illustrate that an intermediate representation (i.e., the architecture
model) is necessary, where information is synthesized according to interest criteria through the clustering algorithms, and that occurs before views specification;
otherwise, the implementation of the latter would be very complex.
4.4 Step 3: Metrics transformation and annotations
The MDRE expert has to carry out the following three tasks in order to guarantee the
success of step 3:
1) Development of a metrics transformation that queries a model conforming to
the architecture metamodel and creates measures and measurements in an
output model. This output model is conforming to the metrics metamodel.
2) Annotation of the architecture metamodel to indicate which structural ele- ments of the
language/technology will be painted, and their appearance. This task has a strong
dependency with the previous one because the measures referenced in the annotations
are influenced by the measures introduced by the metrics transformation. We have
chosen the annotations as the expres- sion mechanism because they are metadata that
do not affect the metamodel nor the related models. Our tool validates that the
annotations have been correctly applied to the architecture metamodel.
Mendivelso et al. Journal of Software Engineering Research and Development (2018) 6:16 Page 9 of 23
3) Generation of a Technology-independent Views Specification (TIVS) model which is
an intermediate representation of the view specification that allows us to be
independent from the concrete graphic framework on top of which the view editor
is built. This model conforms to the TIVS metamodel (see Section 4.4.2) and is
automatically generated from the annotated architecture metamodel by using a
transformation (referred to as Annotation2TIVS in Fig. 1).
The two first tasks are manually carried out by the expert and the latter one is
automated through a transformation.
4.4.1 Metrics transformation
The metrics transformation is responsible for: i) creating measures in the metrics
model; ii) querying the architecture model to obtain measurements for the established measures; and iii) associating architecture elements to measurements in
order to have a trace indicating where do the measurements come from. The
transforma- tion output is a model conforming to the metamodel shown in Fig. 3.
To design this metamodel, we took inspiration from the Structured Metrics Metamodel (SMM) which is an OMG specification (OMG n.d.). The main concepts of
SMM are: Measure, Mea- surement and Observation. Measure defines a type of
software metric that can be applied to one or several architectural elements. Measurement contains the concrete value of a metric for a particular architectural element. Observation saves a trace of which tool is used to compute the measurements,
when, and who is the responsible for the observation.
In our Metrics metamodel, we keep the Measure and Measurement concepts and
leave aside the Observation concept. The latter adds no value to our approach, instead
it impacts the metrics model size and the performance of the views render.
In addition, the SMM includes a set of software measures (that extends the con- cept
Measure) but makes no claims about its standardization. Thus, We decided to put
aside these subclasses too and prefer to keep our Metrics metamodel as minimal as
possible. In fact, it is the MDRE expert the one who adds the measures of her interest
at model level. The Semantic Web is a vision about an extension of the existing World Wide Web, which provides software programs with machine-interpretable metadata of the published information and data. In other words, we add further data descriptors to otherwise existing content and data on the Web. As a result, computers are able to make meaningful interpretations similar to the way humans process information to achieve their goals. Linked Open Data (LOD) is structured data modeled as a graph and published in a way that allows interlinking across servers. This was formalized by Tim Berners-Lee in 2006 as the Four rules of linked data:
Use URIs as names for things.
Use HTTP URIs so that people can look up those names.
When someone looks up a URI, provide useful information, using the standards (RDF*, SPARQL).
Include links to other URIs. so that they can discover more things.
LOD enables both people and machines to access data across different servers and interpret its semantics more easily. As a result, the Semantic Web transcends from a space comprising of linked documents to a space comprising of linked information. Which, in turn, empowers the creation of a richly interconnected network of machine-processable meaning.
Linked Open Data includes:
Factual data about specific entities and concepts (e.g., Varna, WW2 or the Global warming theory);
Ontologies – semantic schemata defining:
Classes of objects (e.g., Person, Organization, Location and Document);
Relationship types (e.g., a parent of or a manufacturer of);
Attributes (e.g., the DoB of a person or the population of a geographic region).
Today, there are thousands of datasets published as LOD across different sectors such as encyclopedia, geographic data, government data, scientific database and articles, entertainment, traveling, etc. In Life Sciences alone, there are more than 100 scientific databases published as LOD.
Because of their linking, these datasets form a giant web of data or a Knowledge Graph, which connects a vast amount of descriptions of entities and concepts of general importance. For example, there are several descriptions of the city of Varna (e.g., one derived from Wikipedia, another from Geonames, etc.). Semantic Metadata amounts to semantic tags that are added to regular Web pages in order to better describe their meaning. For instance, the home page of the Bulgarian Institute for Oceanography can be semantically annotated with references to several appropriate concepts and entities, e.g., Varna, Academic Institution and Oceanography.
Such metadata makes it much easier to find Web pages based on semantic criteria. It resolves any potential ambiguity and ensures that when we search for Paris (the capital of France), we will not get pages about Paris Hilton.
If we want to have a well-determined relationship between the subject of the Web page and the corresponding page or document, it is best to use one of the structured data metadata schemes. Currently, the most popular such scheme is Schema.org, which was established by Google, Yahoo, Microsoft and Yandex. According to a recent study of the University of Mannheim, in 2015, 30% of the Web pages contained Semantic Web Metadata.
Cloud computing is the delivery of on-demand computing services -- from applications to storage and processing power -- typically over the internet and on a pay-as-you-go basis.
How does cloud computing work?
Rather than owning their own computing infrastructure or data centers, companies can rent access to anything from applications to storage from a cloud service provider.
One benefit of using cloud computing services is that firms can avoid the upfront cost and complexity of owning and maintaining their own IT infrastructure, and instead simply pay for what they use, when they use it.
In turn, providers of cloud computing services can benefit from significant economies of scale by delivering the same services to a wide range of customers.
Cloud computing services cover a vast range of options now, from the basics of storage, networking, and processing power through to natural language processing and artificial intelligence as well as standard office applications. Pretty much any service that doesn't require you to be physically close to the computer hardware that you are using can now be delivered via the cloud.
What are examples of cloud computing internet?
Cloud computing underpins a vast number of services. That includes consumer services like Gmail or the cloud back-up of the photos on your smartphone, though to the services which allow large enterprises to host all their data and run all of their applications in the cloud. Netflix relies on cloud computing services to run its its video streaming service and its other business systems too, and have a number of other organisations.
Cloud computing is becoming the default option for many apps: software vendors are increasingly offering their applications as services over the internet rather than standalone products as they try to switch to a subscription model. However, there is a potential downside to cloud computing, in that it can also introduce new costs and new risks for companies using it.
Why is it called cloud computing cloud fundamental concept behind cloud computing is that the location of the service, and many of the details such as the hardware or operating system on which it is running, are largely irrelevant to the user. It's with this in mind that the metaphor of the cloud was borrowed from old telecoms network schematics, in which the public telephone network (and later the internet) was often represented as a cloud to denote that the just didn't matter -- it was just a cloud of stuff. This is an over-simplification of course; for many customers location of their services and data remains a key issue.'
Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.
The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers learn automatically without human intervention or assistance and adjust actions accordingly.
But, using the classic algorithms of machine learning, text is considered as a sequence of keywords; instead, an approach based on semantic analysis mimics the human ability to understand the meaning of a text.
Some machine learning methods Machine learning artificial intelligence algorithms are often categorized as supervised or unsupervised.
Supervised machine learning algorithms can apply what has been learned in the past to new data using labeled examples to predict future events. Starting from the analysis of a known training dataset, the learning algorithm produces an inferred function to make predictions about the output values. The system is able to provide targets for any new input after sufficient training. The learning algorithm can also compare its output with the correct, intended output and find errors in order to modify the model accordingly.
In contrast, unsupervised machine learning algorithms are used when the information used to train is neither classified nor labeled. Unsupervised learning studies how systems can infer a function to describe a hidden structure from unlabeled data. The system doesn’t figure out the right output, but it explores the data and can draw inferences from datasets to describe hidden structures from unlabeled data.
Semi-supervised machine learning algorithms supervised or unsupervised fall somewhere in between supervised and unsupervised learning, since they use both labeled and unlabeled data for training – typically a small amount of labeled data and a large amount of unlabeled data. The systems that use this method are able to considerably improve learning accuracy. Usually, semi-supervised learning is chosen when the acquired labeled data requires skilled and relevant resources in order to train it / learn from it. Otherwise, acquiring unlabeled data generally doesn’t require additional resources.
Reinforcement learning machine learning algorithms is a learning method that interacts with its environment by producing actions and discovers errors or rewards. Trial and error search and delayed reward are the most relevant characteristics of reinforcement learning. This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance. Simple reward feedback is required for the agent to learn which action is best; this is known as the reinforcement signal.
Machine learning enables analysis of massive quantities of data. While it generally delivers faster, more accurate results in order to identify profitable opportunities or dangerous risks, it may also require additional time and resources to train it properly. Combining machine learning with AI and cognitive technologies can make it even more effective in processing large volumes of information.
Data analytics is the science of analyzing raw data in order to make conclusions about that information. Many of the techniques and processes of data analytics have been automated into mechanical processes and algorithms that work over raw data for human consumption.
Data analytics techniques can reveal trends and metrics that would otherwise be lost in the mass of information. This information can then be used to optimize processes to increase the overall efficiency of a business or system.
Understanding Data Analytics
Data analytics is a broad term that encompasses many diverse types of data analysis. Any type of information can be subjected to data analytics techniques to get insight that can be used to improve things.
For example, manufacturing companies often record the runtime, downtime, and work queue for various machines and then analyze the data to better plan the workloads so the machines operate closer to peak capacity.
Data analytics can do much more than point out bottlenecks in production. Gaming companies use data analytics to set reward schedules for players that keep the majority of players active in the game. Content companies use many of the same data analytics to keep you clicking, watching, or re-organizing content to get another view or another click.
The process involved in data analysis involves several different steps:
The first step is to determine the data requirements or how the data is grouped. Data may be separated by age, demographic, income, or gender. Data values may be numerical or be divided by category.
The second step in data analytics is the process of collecting it. This can be done through a variety of sources such as computers, online sources, cameras, environmental sources, or through personnel.
Once the data is collected, it must be organized so it can be analyzed. Organization may take place on a spreadsheet or other form of software that can take statistical data.
The data is then cleaned up before analysis. This means it is scrubbed and checked to ensure there is no duplication or error, and that it is not incomplete. This step helps correct any errors before it goes on to a data analyst to be analyzed.
Data analytics is important because it helps businesses optimize their performances. Implementing it into the business model means companies can help reduce costs by identifying more efficient ways of doing business and by storing large amounts of data.
Descriptive analytics describes what has happened over a given period of time. Have the number of views gone up? Are sales stronger this month than last
Diagnostic analytics focuses more on why something happened. This involves more diverse data inputs and a bit of hypothesizing. Did the weather affect beer sales?Did that latest marketing campaign impact sales?
Predictive analytics moves to what is likely going to happen in the near term. What happened to sales the last time we had a hot summer? How many weather models predict a hot summer this year?
Prescriptive analytics suggests a course of action. If the likelihood of a hot summer is measured as an average of these five weather models is above 58%, we should add an evening shift to the brewery and rent an additional tank to increase output.
Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.
The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.
When most people hear the term artificial intelligence, the first thing they usually think of is robots. That's because big-budget films and novels weave stories about human-like machines that wreak havoc on Earth. But nothing could be further from the truth.
Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception.
As technology advances, previous benchmarks that defined artificial intelligence become outdated. For example, machines that calculate basic functions or recognize text through optimal character recognition are no longer considered to embody artificial intelligence, since this function is now taken for granted as an inherent computer function.
AI is continuously evolving to benefit many different industries. Machines are wired using a cross-disciplinary approach based in mathematics, computer science, linguistics, psychology, and more.
The applications for artificial intelligence are endless. The technology can be applied to many different sectors and industries. AI is being tested and used in the healthcare industry for dosing drugs and different treatment in patients, and for surgical procedures in the operating room.
Other examples of machines with artificial intelligence include computers that play chess and self-driving cars. Each of these machines must weigh the consequences of any action they take, as each action will impact the end result. In chess, the end result is winning the game. For self-driving cars, the computer system must account for all external data and compute it to act in a way that prevents a collision.
Artificial intelligence also has applications in the financial industry, where it is used to detect and flag activity in banking and finance such as unusual debit card usage and large account deposits—all of which help a bank's fraud department. Applications for AI are also being used to help streamline and make trading easier. This is done by making supply, demand, and pricing of securities easier to estimate.	
Urban computing brings powerful computational techniques to bear on such urban challenges as pollution, energy consumption, and traffic congestion. Using today's large-scale computing infrastructure and data gathered from sensing technologies, urban computing combines computer science with urban planning, transportation, environmental science, sociology, and other areas of urban studies, tackling specific problems with concrete methodologies in a data-centric computing framework. This authoritative treatment of urban computing offers an overview of the field, fundamental techniques, advanced models, and novel applications.
Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making. Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network.
Deep learning has evolved hand-in-hand with the digital era, which has brought about an explosion of data in all forms and from every region of the world. This data, known simply as big data, is drawn from sources like social media, internet search engines, e-commerce platforms, and online cinemas, among others. This enormous amount of data is readily accessible and can be shared through fintech applications like cloud computing.
However, the data, which normally is unstructured, is so vast that it could take decades for humans to comprehend it and extract relevant information. Companies realize the incredible potential that can result from unraveling this wealth of information and are increasingly adapting to AI systems for automated support.
One of the most common AI techniques used for processing big data is machine learning, a self-adaptive algorithm that gets increasingly better analysis and patterns with experience or with newly added data.
If a digital payments company wanted to detect the occurrence or potential for fraud in its system, it could employ machine learning tools for this purpose. The computational algorithm built into a computer model will process all transactions happening on the digital platform, find patterns in the data set, and point out any anomaly detected by the pattern.
Deep learning, a subset of machine learning, utilizes a hierarchical level of artificial neural networks to carry out the process of machine learning. The artificial neural networks are built like the human brain, with neuron nodes connected together like a web. While traditional programs build analysis with data in a linear way, the hierarchical function of deep learning systems enables machines to process data with a nonlinear approach.
Using the fraud detection system mentioned above with machine learning, one can create a deep learning example. If the machine learning system created a model with parameters built around the number of dollars a user sends or receives, the deep-learning method can start building on the results offered by machine learning.
Each layer of its neural network builds on its previous layer with added data like a retailer, sender, user, social media event, credit score, IP address, and a host of other features that may take years to connect together if processed by a human being. Deep learning algorithms are trained to not just create patterns from all transactions, but also know when a pattern is signaling the need for a fraudulent investigation. The final layer relays a signal to an analyst who may freeze the user’s account until all pending investigations are finalized.
Deep learning is used across all industries for a number of different tasks. Commercial apps that use image recognition, open-source platforms with consumer recommendation apps, and medical research tools that explore the possibility of reusing drugs for new ailments are a few of the examples of deep learning incorporation.
What is the Internet of Things?
In the broadest sense, the term IoT encompasses everything connected to the internet, but it is increasingly being used to define objects that "talk" to each other. "Simply, the Internet of Things is made up of devices – from simple sensors to smartphones and wearables – connected together," Matthew Evans, the IoT programme head at techUK, says.
By comb	ining these connected devices with automated systems, it is possible to "gather information, analyse it and create an action" to help someone with a particular task, or learn from a process. In reality, this ranges from smart mirrors to beacons in shops and beyond.
"It's about networks, it's about devices, and it's about data," Caroline Gorski, the head of IoT at Digital Catapult explains. IoT allows devices on closed private internet connections to communicate with others and "the Internet of Things brings those networks together. It gives the opportunity for devices to communicate not only within close silos but across different networking types and creates a much more connected world."
“Cognitive computer vision is concerned with integration and control of vision
systems using explicit but not necessarily symbolic models of context, situation
and goal-directed behaviour. Cognitive vision implies functionalities for knowledge representation, learning, reasoning about events & structures, recognition
and categorization, and goal specification, all of which are concerned with the
semantics of the relationship between the visual agent and its environment.’
Big data analytics is the use of advanced analytic techniques against very large, diverse data sets that include structured, semi-structured and unstructured data, from different sources, and in different sizes from terabytes to zettabytes.
Big data is a term applied to data sets whose size or type is beyond the ability of traditional relational databases to capture, manage and process the data with low latency. Big data has one or more of the following characteristics: high volume, high velocity or high variety. Artificial intelligence (AI), mobile, social and the Internet of Things (IoT) are driving data complexity through new forms and sources of data. For example, big data comes from sensors, devices, video/audio, networks, log files, transactional applications, web, and social media — much of it generated in real time and at a very large scale.
Analysis of big data allows analysts, researchers and business users to make better and faster decisions using data that was previously inaccessible or unusable. Businesses can use advanced analytics techniques such as text analytics, machine learning, predictive analytics, data mining, statistics and natural language processing to gain new insights from previously untapped data sources independently or together with existing enterprise data.
Applied operations research at the Department of Management Sciences provides quantitative tools to model complex decision making problems in manufacturing and service industries in modern global economy. Our faculty are experts in optimization, stochastic processes, Markov decision processes, data analytics, and decisions analysis with focus areas in health care, supply chain management and logistics, revenue management and pricing, energy, and manufacturing.
Applied Operations research (OR) encompasses the development and the use of a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, Markov decision processes, econometric methods, data envelopment analysis, neural networks, expert systems, decision analysis, and the analytic hierarchy process.
ad hoc and mobile networks is a decentralized type of wireless network The network is ad hoc because it does not rely on a pre-existing infrastructure, such as routers in wired networks or access points in managed (infrastructure) wireless networks. Instead, each node participates in routing by forwarding data for other nodes, so the determination of which nodes forward data is made dynamically on the basis of network connectivity and the routing algorithm in use.
In the Windows operating system, ad hoc is a communication mode (setting) that allows computers to directly communicate with each other without a router. Wireless mobile ad hoc networks are self-configuring, dynamic networks in which nodes are free to move.
application specific processors  is a component used in system-on-a-chip design. The instruction set of an ASIP is tailored to benefit a specific application. This specialization of the core provides a tradeoff between the flexibility of a general purpose CPU and the performance of an ASIC.
Some application specific processors have a configurable instruction set. Usually, these cores are divided into two parts: static logic which defines a minimum ISA (instruction-set architecture) and configurable logic which can be used to design new instructions. The configurable logic can be programmed either in the field in a similar fashion to an Field-programmable gate array (FPGA) or during the chip synthesis.
application specific processors can be used as an alternative of hardware accelerators for baseband signal processing[1] or video coding.[2] Traditional hardware accelerators for these applications suffer from inflexibility. It is very difficult to reuse the hardware datapath with handwritten finite-state machines (FSM). The retargetable compilers of ASIPs help the designer to update the program and reuse the datapath. Typically, the ASIP design is more or less dependent on the tool flow because designing a processor from scratch can be very complicated. There are some commercial tools to design ASIPs, for example Processor Designer from Synopsys. There is an open source tool as well, TTA-based co-design environment (TCE).
The cybersecurity industry is changing and evolving at a rapid pace. Technology is providing options that didn’t exist in the past and creating new solutions, better insights and lower costs for end users.
Additionally, many of these solutions allow for faster, less intrusive installations and remote trouble-shooting, which create efficiencies for customers as well as security providers. One of the best examples of this technology improvement is wireless video surveillance and the ecosystem of wireless sensors.
What Is Wireless Video Surveillance Wireless video systems communicate via the Internet through a WiFi signal. This communication could be from a wireless router on-premise or through a cellular communicator. The video cameras themselves are not hardwired to the local routers, servers or storage equipment. Most wireless video systems utilize cameras that are connected to traditional power sources.
However, true wireless video cameras — meaning battery powered — have grown in popularity in the residential segment but have not been as widely adopted in the commercial segment due to their limitations (more on that later).
From tablets to control panels, wireless capability is cropping up in products in a variety of industries, and security is no exception. Wireless technology is readily available and inexpensive to incorporate into devices, which is why manufacturers are seizing the opportunity to produce and market a variety of Wi-Fi-enabled security products. But manufacturers aren’t the only ones who benefit from wireless; an increasing number of dealers and integrators are riding the wireless wave.
One reason for this shift in technological paradigm stems from installation procedures. Without having to run long stretches of cabling from the cameras, the installation becomes a much quicker job, requiring fewer hours of labor. Added mobility and flexibility are other appealing qualities of wireless cameras. In other words, a wireless camera can be easily installed, uninstalled, and reinstalled elsewhere on a premises, provided there is a viable power source and signal.
Although some wireless cameras utilize a mesh network, a Wi-Fi network is used most commonly to transmit data. Wi-Fi-enabled products adhere to at least one of three standards set by the Institute of Electrical and Electronics Engineers (IEEE): 802.11g, 802.11n and 802.11ac. The most commonly held standard today is 802.11g, which is able to transfer up to 54 megabits per second and has been implemented worldwide. Wi-Fi, in actuality, is an unlicensed radio frequency that transmits at a power level less than 1 milliwatt. This means it is open for anyone to use and is what has prompted its widespread usage.
A wireless camera has all of the equipment needed to transfer video already built into it. To transfer video, it is first compressed to a smaller size that facilitates transmission over the network — similar to how one might compress several photos into a .zip file to send in an email. Next, the camera encodes the video data into a series of 0s and 1s that is wirelessly transferred via radio waves (Wi-Fi) to a digital destination, such as an NVR or other data storage appliance.
Even though wireless video transmission is a growing trend in the security industry, some are hesitant to be swept up into the Wi-Fi current. As with any technology, wireless has its pros and cons. One real concern is the matter of the Wi-Fi signal: if there’s no signal, there’s no video. There is also the risk that a signal can be disrupted by changes in the environment surrounding either the camera transmitting the signal or the signal-receiving device. Metallic additions to a building, even a small change such as adding foil-backed wallpaper to a room, are known to disrupt a video transmission signal. In general, Wi-Fi coverage is more subject to change, and therefore, so is camera performance. The good news is that wireless camera systems are easily repositioned to address these kinds of problems. Still, regular testing is recommended to ensure that the signal is able to reach its destination at critical moments.
Although Wi-Fi might be what most often comes to mind when talking about wireless cameras, video transmission isn’t the only way in which a camera might be considered “wireless.” Battery-powered and solar-powered cameras with battery backup are available to meet certain customer needs. Most commonly used in situations where a conventional power source isn’t practical or available, solar-powered cameras operate by charging a battery via a solar panel during the day, allowing the camera to function at all hours.
Solely battery-powered cameras, while in the truest sense of the word are not yet considered to be surveillance cameras, still have found a place in the security industry. Most often motion-based, these cameras will record a few seconds of video upon sensing an intrusion (or other designated event). Once recorded, the cameras send video to a designated destination, such as a control panel. However, one company — RSI Videofied — is about to change the way we think about battery-powered cameras. A streaming wireless battery camera is in the works from RSI Videofied — the StreamingViewer — and is scheduled to ship sometime in 2015.
No matter how one thinks of wireless, it’s a growing trend that’s here to stay. More consumers are coming to expect wireless capabilities from the technology they purchase, and it’s up to dealers and integrators to meet those expectations.
Software engineering is a branch of computer science which includes the development and building of computer systems software and applications software. Computer systems software is composed of programs that include computing utilities and operations systems. Applications software consists of user-focused programs that include web browsers, database programs, etc.
There is a lot of investment going into software engineering at the moment due to the increasing reliance on mobile technology, venture capital-backed start-ups, the growing complexity of technology, and emerging industries. The demand for skilled and qualified software engineers seems to have no end. This demand is strengthened by a changing economic landscape and fueled by the need for technology solutions. With billions of physical devices around the world that are now connected to the internet and that are collecting and sharing data, all industries are quickly becoming technology driven industries.
Software engineers have extensive knowledge of programming languages, software development, and computer operating systems, and they apply engineering principles to software creation. By applying these engineering principles to every stage of the development process, from requirements analysis to the software process, they can create customized systems for individual clients. Just as a civil engineer will make sure that a bridge has a solid foundation, a software engineer will also begin with a thorough study of requirements and work through the development process in a systematic way.
software engineering When working with a client, a software engineer will typically analyze the client's needs, then design, test, and develop the computer software in order to meet those needs. They are experts in computing systems, software structure, and recognizing the limitations of the existing hardware. The process is complicated and intricate, therefore the use of diagrams, flowcharts, and the creation of algorithms to tell the computer what to do are created. Converting these instructions into a computer language (coding/programming) is usually the responsibility of a computer programmer.
Network security is a broad term that covers a multitude of technologies, devices and processes. In its simplest term, it is a set of rules and configurations designed to protect the integrity, confidentiality and accessibility of computer networks and data using both software and hardware technologies. Every organization, regardless of size, industry or infrastructure, requires a degree of network security solutions in place to protect it from the ever-growing landscape of cyber threats in the wild today.
Today's network architecture is complex and is faced with a threat environment that is always changing and attackers that are always trying to find and exploit vulnerabilities. These vulnerabilities can exist in a broad number of areas, including devices, data, applications, users and locations. For this reason, there are many network security management tools and applications in use today that address individual threats and exploits and also regulatory non-compliance. When just a few minutes of downtime can cause widespread disruption and massive damage to an organization's bottom line and reputation, it is essential that these protection measures are in place.
Physical security controls are designed to prevent unauthorized personnel from gaining physical access to network components such as routers, cabling cupboards and so on. Controlled access, such as locks, biometric authentication and other devices, is essential in any organization.
Technical Network Security
Technical security controls protect data that is stored on the network or which is in transit across, into or out of the network. Protection is twofold; it needs to protect data and systems from unauthorized personnel, and it also needs to protect against malicious activities from employees.
Administrative Network Security
Administrative security controls consist of security policies and processes that control user behavior, including how users are authenticated, their level of access and also how IT staff members implement changes to the infrastructure.
Types of network security
We have talked about the different types of network security controls. Now let's take a look at some of the different ways you can secure your network.
Network Access Control
To ensure that potential attackers cannot infiltrate your network, comprehensive access control policies need to be in place for both users and devices. Network access control (NAC) can be set at the most granular level. For example, you could grant administrators full access to the network but deny access to specific confidential folders or prevent their personal devices from joining the network.
Antivirus and Antimalware Software
Antivirus and antimalware software protect an organization from a range of malicious software, including viruses, ransomware, worms and trojans. The best software not only scans files upon entry to the network but continuously scans and tracks files.
Firewall Protection
Firewalls, as their name suggests, act as a barrier between the untrusted external networks and your trusted internal network. Administrators typically configure a set of defined rules that blocks or permits traffic onto the network. For example, Forcepoint's Next Generation Firewall (NGFW) offers seamless and centrally managed control of network traffic, whether it is physical, virtual or in the cloud.
Virtual Private Networks
Virtual private networks (VPNs) create a connection to the network from another endpoint or site. For example, users working from home would typically connect to the organization's network over a VPN. Data between the two points is encrypted and the user would need to authenticate to allow communication between their device and the network. Forcepoint's Secure Enterprise SD-WAN allows organizations to quickly create VPNs using drag-and-drop and to protect all locations with our Next Generation Firewall solution.
Network security for businesses and consumers
Network security should be a high priority for any organization that works with networked data and systems. In addition to protecting assets and the integrity of data from external exploits, network security can also manage network traffic more efficiently, enhance network performance and ensure secure data sharing between employees and data sources.
There are many tools, applications and utilities available that can help you to secure your networks from attack and unnecessary downtime. Forcepoint offers a suite of network security solutions that centralize and simplify what are often complex processes and ensure robust network security is in place across your enterprise.
If computers could loam from experiencetheirusefulness
would be increased. When I write a clumsy program for a contemporary computer a thousand runs on the
machine do not re-educate my handiwork. On every execution, each time-wasting blemish and crudity, each
needless test and redundant evaluation, is meticulously reproduced.
Attempts to computerize learning processes date back little more than 10 yr. The most significant early milestone was A. L. Samuel's studyl using the game of checkers (draughts). Samuel devised detailedprocedures both of "rote-learning" and "learning by generalization".
When coupled with efficient methods of look ahead and search, these procedures enabled the computer to raise itself by prolongedpractice from the status of a beginner to that of a tournament player. Hence there now exists a checkers program which can learn through experience of checkers to play better checkers. What does not yet exist is any way of providing in general that all our
programs will automatically raise the efficacy of their own execution.
Artificial intelligence (AI) is an important technology that supports daily social life and economic
activities. It contributes greatly to the sustainable growth of Japan's economy and solves various social
problems. In recent years, AI has attracted attention as a key for growth in developed countries such
as Europe and the United States and developing countries such as China and India. The attention has
been focused mainly on developing new artificial intelligence information communication technology
(ICT) and robot technology (RT). Although recently developed AI technology certainly excels in
extracting certain patterns, there are many limitations. Most ICT models are overly dependent on big
data, lack a self-idea function, and are complicated. In this paper, rather than merely developing nextgeneration artificial intelligence technology, we aim to develop a new concept of general-purpose
intelligence cognition technology called “Beyond AI”. Specifically, we plan to develop an intelligent
learning model called “Brain Intelligence (BI)” that generates new ideas about events without having
experienced them by using artificial life with an imagine function. We will also conduct
demonstrations of the developed BI intelligence learning model on automatic driving, precision
medical care, and industrial robots From SIRI [1] to AlphaGo [2], artificial intelligence (AI) is developing rapidly. While science
fiction often portrays AI as robots with human-like characteristics, AI can encompass anything from
e-Commerce prediction algorithms to IBM’s Watson machines [3]. However, artificial intelligence
today is properly known as weak AI, which is designed to perform a special task (e.g., only facial
recognition or only internet searches or only driving a car). While weak AI may outperform humans
at a specific task, such as playing chess or solving equations, general AI would outperform humans at
nearly every cognitive task.
In recent years, the US government has supported basic research on AI, which is centered on robots
and pattern recognition (voice, images, etc.). Microsoft has announced real-time translation robots and
innovative image recognition technologies [4]. Amazon uses artificial intelligence for autonomous 
robots in delivery systems [5]. Facebook has also developed facial recognition technology based on
artificial intelligence called “DeepFace” [6]. Robots and artificial intelligence are being actively
studied in university institutions in the United States. Innovative technologies, such as corporate
cooperation and deep learning, are emerging. The robot car developed by the Artificial Intelligence
Laboratory at Stanford University has set a faster time than an active racer [7]. The Computer Science
and Artificial Intelligence Laboratory at Massachusetts Institute of Technology has developed a
cleaning robot and a four-foot walking robot [8].
Meanwhile, AI is the main technology expected to improve Japanese ICT’s innovation and robot
technology in the near future. ICT in Japan has rapidly advanced in recent years. To secure Japan’s
status as a world-class “technological superpower”, the Japanese government has formulated projects
such as the “Science and Technology Basic Plan [9]” and the “Artificial Intelligence Technology
Conference [10]”. Japan is expecting to utilize state-of-the-art artificial intelligence and robots to solve
various problems.
However, through some research, we found that recent artificial intelligence technologies have
many limitations. In the following, we list some representative limitations and analyze the reasons
why recent AI cannot break through these inherent disadvantages. 
In recent years, artificial intelligence technologies have developed dramatically due to improvement
in the processing capacity of computers and the accumulation of big data. However, the results of
current artificial intelligence technologies remain limited to specific intellectual areas, such as image
recognition, speech recognition, and dialogue response. That is, current AI is a specialized type of
artificial intelligence acting intellectually in a so-called individual area (see Figure 1). Examples
include techniques such as Convolutional Neural Networks (CNN) or Deep Residual Learning
(ResNet) for visual recognition, Recurrent Neural Networks (RNN) or Deep Neural Networks (DNN)
for speech recognition, and Represent Learning (RL) for dialogue understanding. All of these are a
part of the intellectual work carried out by each area of the human brain; they are only a substitute and
do not perform all of the functions of the human brain. In other words, AI has not been able to
cooperate with whole-brain functions such as self-understanding, self-control, self-consciousness and
self-motivation. Specifically, we conclude that the limitations of the recent artificial intelligence
technologies are the following
Virtual reality uses simple devices and advanced systems that can network with humans. Virtual
reality is a computer-generated simulation of a 3D environment that can be interacted with in a
seemingly real manner. Artificial intelligence will be used in augmented reality for future remote eHealth 
Currently, research used in pattern recognition and classification is primarily supported by very 
large data sets. Few approaches look for providing a solution better than existing big-data processing
platforms, which usually runs over a large-scale commodity CPU cluster. Moreover, GPUs seem to
the best platforms to train AI networks [28]. However, recent platforms are worse than the human
brain in processing perception and require large amounts of space and energy. To this end, Rajat et al.
[29] trained a DBN model with 100 million parameters using an Nvidia GTX280 graphics card with
240 cores. Adam et al. [30] proposed a COTS HPC deep neural network training system. Google
developed DistBelief [31], which uses thousands of CPUs to train the deep neural network; see Figure
2. Microsoft developed project Adam [32] to use fewer machines for training. Other sample vendors,
such as Qualcomm’s Zeroth platform [33], IBM’s Truenorth [34], and Manchester University’s
SpiNNaker [35] are also in development. Besides, there are also some software packages for deep
learning. These packages include Tensorflow, Theano, Torch/PyTorch, MxNet, Caffe as well as high level package Keras. It would also be good to mention Google's TPU when mentioning the hardware
platforms. Agents, or more precisely intelligent agents, are a novel paradigm for software applications development, supporting the simulation of complex individual interactions. Moreover, agent-based computing has been welcomed as the latest paradigm shift in software development as well as the new software revolution. Presently, agents are one of the main fields of interest in computer science, artificial intelligence (AI), and complex system theory.
Intelligent (or rational) agents are used in a wide multiplicity of applications, ranging from relatively small systems, e.g. email filters, to large complex systems, such as air traffic control, bird flocking, or human social behaviour. Apparently, it may look that such particularly unlike types of system cannot have much in common. Still, nothing can be more deceitful, as in both the key concept is the agent.
Before addressing the issue of agent-based systems (ABS) development, one should try to define what terms ‘agent’ and ABS mean. Regrettably, there is a lack of commonly accepted definitions about key concepts in agent-based computing. Actually, there is no genuine agreement on the definition of the term agent.
As Russell and Norvig [1] stated an agent is anything that can be considered able to perceive its environment through sensors and act on this environment through actuators. To Macal [2], an agent shall have the following characteristics: (i) be identifiable—a discrete individual with a set of features and rules (mathematical or logic) that govern behaviour and decision-making capacity; (ii) be located—settled in an environment with which it interacts and also in which interacts with other agents; (iii) be goal-driven; (iv) be self-contained; and (v) be flexible, and have the ability to learn and adapt its behaviour through time-based experiences.
Most authors agree that although there are multiple definitions of the term ‘agent’, several attributes can be pointed out such as heterogeneity, autonomy, capacity to process and exchange information, follow if-then rules, goal-driven, and deductive code-based units, with boundary and state. There are also core behaviours: mobility, interaction, adaptation, and bounded rationality [3].
Agents are not aggregated into homogeneous pools. Rather, agents are heterogeneous with different attributes, behaviours and rules, which may differ in multiple ways (e.g. social network, individual preferences), and over time. For instance, one can model groups of residents in a specific neighbourhood but these individuals can have heterogeneous characteristics such as age, gender, income, and living preferences, although associating with the same group.
Agents are autonomous entities that are not subjected to the influence of external direction. They are developed over a bottom-up approach and have the capacity for processing information, while sharing it with other agents, through individual-based interaction that does not suffer top-down control. However, when a new agent enters the simulation, its actions can be conditioned by pre-existing norms that have been instituted through earlier agent interactions and persisted through time steps. These interactions can be expressed by the interchange of data from one agent to another. In such a way, micro and macro level will typically coevolve without pre-defined upper level controllers (i.e., bottom-up approach).
An agent may be goal-driven and takes independent actions to reach its goals. Thus, agents compare behaviour outcomes to its goals and adapt responses in the future. An agent’s behaviour can be described by simple if-then rules that used to describe the theoretical assumptions of agent behaviour, in the form of computational procedures that lead to goal achievement. These procedures constitute a plan for achieving agents’ objectives.
ABS is one where an agent is used as key abstraction. Theoretically, an ABS could be conceptualized in terms of agents and still be implemented without using any software consistent to agents. There is an obvious parallelism with object-oriented software, where it is fully conceivable to design a system based on objects, and implement it without getting use of object-oriented software. Nevertheless, this would be counterproductive or, at least, unusual. The same happens with ABS, where users expect agents to be designed and implemented using agent paradigm (e.g., using specific agent based software).
One should note that an ABS may have any non-zero amount of agents. A multi-agent system (MAS), designed and implemented by means of several interacting agents, is more general and pointedly more complex than the unitary (single case) agent. In real world, there are various number of situations where the single-agent case is suitable. A good example is the expert assistant, where an agent acts like an expert assistant to a user attempting to fulfil some task on a computer.
Multi agent systems is a computer-based environment made of multiple interacting intelligent agents. MAS are preferably used in solving problems that are difficult (or impossible) for an individual agent. As with agents, there is no categorical definition of MAS so let us focus on one that is relatively consensual. In Stone and Veloso [4], MAS is defined as ‘a loosely coupled network of problem-solving entities (agents) that work together to find answers to problems that are beyond the individual capabilities or knowledge of each entity (agent)’.
As in Multi agent systems, agent-based model (ABM) also consists of interacting agents within a specific environment. ABM is known by different names due to its wide variety of applications, which could refer to entirely diverse methodologies. It can also be called a multi-agent system (MAS) or agent-based system (ABS).
In a computer science (e.g. artificial intelligence), ABM usually states a computer-based method for studying the (inter)actions of a set of autonomous entities. In non-computing–related scientific domains as in social sciences, ABM could refer to an actor in the social world and be called agent-based social simulation (ABSS). Davidsson [5], using different combinations of focus areas (e.g. agent-based computing, computer simulation, and social sciences), further subdivides ABSS into three categories: (i) social aspects of agent systems (SAAS); (ii) multi-agent based simulation (MABS); and (iii) social simulation (SocSim).
In other domains (e.g. transportation ecological science or life science), ABM mostly refer to an individual-based model or a self-sufficient computing method. Although ABM is a wide ranging paradigm applied in totally different manners in all types of scientific domains, eventually all its subtle differences meet together under the domain of agent-based computing [6].
Even though there is a significant overlay, MAS not necessarily means the same as ABM. The objective of an ABM is to search for descriptive insights into the agents’ (not necessarily intelligent) collective behaviour following simple rules (typical of natural systems) rather than solving particular engineering problems. ABM is more often used in the sciences, whereas MAS is frequently applied in engineering- and technology-related issues [6]. Hereafter the designation MAS will be use as a general term covering all agent related semantics discussed in previous paragraphs.
ad hoc and mobile networks are multi hop networks where nodes can be stationary or mobile; and they are formed on a dynamic basis. They allow people to perform tasks efficiently by offering unprecedented levels of access to information. In mobile ad-hoc networks, topology is highly dynamic and random; and in addition, the distribution of nodes and their capability of self-organizing play an important role. Their main characteristics can be summarized as follows: The topology is highly dynamic and frequent changes in the topology may be hard to predict; Mobile ad-hoc networks are based on wireless links, which will continue to have a significantly lower capacity than their wired counterparts; Physical security is limited due to the wireless transmission; Mobile ad-hoc networks are affected by higher loss rates, and can present higher delays and jitter than fixed networks due to the wireless transmission; and Mobile ad-hoc network nodes rely on batteries or other exhaustible means for their energy. As a result, energy savings are an important system design criterion. Furthermore, nodes have to be power-aware: the set of functions offered by a node depends on its available power (CPU, memory, etc.).
A well-designed architecture for mobile ad-hoc networks involves all networking layers, ranging from the physical to the application layer. Power management is of paramount importance; and general strategies for saving power need to be addressed, as well as adaptation to the specifics of nodes of general channel and source coding methods, of radio resource management and multiple accesses. In mobile ad-hoc networks, with the unique characteristic of being totally independent from any authority and infrastructure, there is a great potential for the users. In fact, roughly speaking, two or more users can become a mobile ad-hoc network simply by being close enough to meet the radio constraints, without any external intervention.
Routing problems have been addressed through research; where routing protocols between any pair of nodes within an ad-hoc network can be difficult because the nodes can move randomly and can also join or leave the network. This means that an optimal route at a certain time may not work seconds later.
Two of the best multicast protocols to be adopted are MAODV (Multicast Ad-hoc on-demand Distance Vector Routing Protocol) and ODMRP (On Demand Multicast Routing Protocol). The performance measures that were evaluated are the PDR (Packet Delivery Ratio) and the Latency. Previous studies have evaluated these algorithms with respect to the network traffic, the node speed, the area and the antenna range for different simulation scenarios. In general, MAODV performs better for high traffic. ODMRP performs better for large areas and high node speeds but poorer for small antenna ranges. Therefore, MAODV and its derivative AODV ALMA will be adopted in this project. A number of technical challenges are faced today due to the heterogeneous, dynamic nature of this hybrid MANET. The hybrid routing scheme AODV ALMA can act simultaneously combining mobile agents to find path to the gateway and on-demand distance vector approach to find path in local MANET is one of the unique solution. An adaptive gateway discovery mechanism based on mobile agents making use of pheromone value, pheromone decay time and balance index is used to estimate the path and next hop to the gateway. The mobile nodes automatically configure the address using mobile agents first selecting the gateway and then using the gateway prefix address. The mobile agents are also used to track changes in topology enabling high network connectivity with reduced delay in packet transmission to Internet.
Clustering is an effective technique for node management in a MANET. Cluster formation involves election of a mobile node as Cluster head to control the other nodes in the newly formed cluster. The connections between nodes and the cluster head changes rapidly in a mobile ad-hoc network. Thus cluster maintenance is also essential. Prediction of mobility-based cluster maintenance involves the process of finding out the next position that a mobile node might take based on the previous locations it visited. The overhead can be reduced in communication by predicting mobility of node using linear autoregression and cluster formation.
Important top 10 algorithms and data structure for competitive coding Topics Graph algorithms Dynamic programming Searching and Sorting: Number theory and Other Mathematical Geometrical and Network Flow Algorithms and Data Structure algorithms and data structure algorithms 
Biometric cryptography is a group of emerging technologies that securely bind a digital key to a biometric or generate a digital key from the biometric, so that no biometric image or template is stored. It must be computationally difficult to retrieve either the key or the biometric from the stored BE template, which is also called “helper data.” The key will be recreated only if the genuine biometric sample is presented on verification. The output of the BE authentication is either a key (correct or incorrect) or a failure message.
Unlike conventional cryptography, this “encryption/ decryption” process is fuzzy because of the natural variability of the biometrics. BE conceptually differs from other systems that encrypt biometric images or templates using conventional encryption, or store a cryptographic key and release it upon successful biometric... Biometric Cryptography, also called Biometric Tokenization, refers to an authentication or other access system that combines inherence factors with public-key infrastructure (PKI). In particular, biometric cryptography is set up to take advantage of the convenience of authentication via fingerprint, face, eye, voice, palm, etc. — with none of the risks posed by having the biometrics take the form of a shared secret.
When a service developing an authentication system selects biometrics as the secret users must present, they face a choice as to where the biometric template is held and matched. Biometric cryptography utilizes a decentralized model (e.g. FIDO UAF) that ensures biometric templates are stored on end-user mobile devices that already have biometric authenticators. In this model, users authenticate to the service by matching their biometric with the template on their device and once it is matched, the device communicates with the service using tokens so biometric information is never transmitted over the wire.
Biometric cryptography enables the service provider to abandon the risks associated with central biometrics storage. The most glaring example of a biometrics breach to date is the US Office of Personnel Management 2015 data breach where millions of biometric templates were stolen among many millions more pieces of personally identifiable information (PII).
A brain-computer interface (BCI) is a computer-based system that acquires brain signals, analyzes them, and translates them into commands that are relayed to an output device to carry out a desired action.
In principle, any type of brain signal could be used to control a BCI system. The most commonly studied signals are electrical signals from brain activity measured from electrodes on the scalp, on the cortical surface, or in the cortex.
A BCI system consists of 4 sequential components: (1) signal acquisition, (2) feature extraction, (3) feature translation, and (4) device output. These 4 components are controlled by an operating protocol that defines the onset and timing of operation, the details of signal processing, the nature of the device commands, and the oversight of performance.
At present, the striking achievements of BCI research and development remain confined almost entirely to the research laboratory. Studies that seek to demonstrate BCI practicality and efficacy for long-term home use by people with disabilities are just beginning.
Brain-computer interfaces may eventually be used routinely to replace or restore useful function for people severely disabled by neuromuscular disorders and to augment natural motor outputs for pilots, surgeons, and other highly skilled professionals. Brain-computer interfaces might also improve rehabilitation for people with strokes, head trauma, and other disorders.
The future of BCIs depends on progress in 3 critical areas: development of comfortable, convenient, and stable signal-acquisition hardware; BCI validation and dissemination; and proven BCI reliability and value for many different user populations.
Brain-computer interfaces (BCIs) acquire brain signals, analyze them, and translate them into commands that are relayed to output devices that carry out desired actions. BCIs do not use normal neuromuscular output pathways. The main goal of BCI is to replace or restore useful function to people disabled by neuromuscular disorders such as amyotrophic lateral sclerosis, cerebral palsy, stroke, or spinal cord injury. From initial demonstrations of electroencephalography-based spelling and single-neuron-based device control, researchers have gone on to use electroencephalographic, intracortical, electrocorticographic, and other brain signals for increasingly complex control of cursors, robotic arms, prostheses, wheelchairs, and other devices. Brain-computer interfaces may also prove useful for rehabilitation after stroke and for other disorders. In the future, they might augment the performance of surgeons or other medical professionals. Brain-computer interface technology is the focus of a rapidly growing research and development enterprise that is greatly exciting scientists, engineers, clinicians, and the public in general. Its future achievements will depend on advances in 3 crucial areas. Brain-computer interfaces need signal-acquisition hardware that is convenient, portable, safe, and able to function in all environments. Brain-computer interface systems need to be validated in long-term studies of real-world use by people with severe disabilities, and effective and viable models for their widespread dissemination must be implemented. Finally, the day-to-day and moment-to-moment reliability of BCI performance must be improved so that it approaches the reliability of natural muscle-based function.
A BCI is a computer-based system that acquires brain signals, analyzes them, and translates them into commands that are relayed to an output device to carry out a desired action. Thus, BCIs do not use the brain's normal output pathways of peripheral nerves and muscles. This definition strictly limits the term BCI to systems that measure and use signals produced by the central nervous system (CNS). Thus, for example, a voice-activated or muscle-activated communication system is not a BCI. Furthermore, an electroencephalogram (EEG) machine alone is not a BCI because it only records brain signals but does not generate an output that acts on the user's environment. It is a misconception that BCIs are mind-reading devices. Brain-computer interfaces do not read minds in the sense of extracting information from unsuspecting or unwilling users but enable users to act on the world by using brain signals rather than muscles. The user and the BCI work together. The user, often after a period of training, generates brain signals that encode intention, and the BCI, also after training, decodes the signals and translates them into commands to an output device that accomplishes the user's intention.
Common sense reasoning is a field of artificial intelligence that aims to help computers understand and interact with people in a more naturally by finding ways to collect these assumptions and teach them to computers. Common Sense Reasoning has been most successful in the field of natural language processing (NLP), though notable work has been done in other areas. This area of machine learning, with its strange name, is starting to quietly infiltrate different applications ranging from text understanding to processing and comprehending what’s in a photo.
Without common sense reasoning, it will be difficult to build adaptable and unsupervised NLP systems in an increasingly digital and mobile world. When we talk to each other and talk online, we try to be as interesting as possible and take advantage of new ways to express things. It’s important to create computers that can keep pace with us.
There’s more to it than one would think. If I asked you if a giraffe would fit in your office, you could answer the question quite easily despite the fact that in all probability you had never pictured a giraffe inhabiting your office, quietly munching on your ficus while your favorite Pandora station plays in the background. This is a perfect example of you not just knowing about the world, but knowing how to apply your world knowledge to things you haven’t thought about before.
The power of common sense systems is that they are highly adaptive, adjusting to topics as varied as restaurant reviews, hiking boot surveys, and clinical trials, and doing so with speed and accuracy. This is because we understand new words from the context they are used in. We use common sense to make guesses at word meanings and then refine those guesses and we’ve built a system that works similarly. Additionally, when we understand complex or abstract concepts, it’s possible we do so by making an analogy to a simple concept, a theory described by George Lakoff in his book, “Metaphors We Live By.” The simple concepts are common sense.
There are two major schools of thought in common-sense reasoning. One side works with more logic-like or rule-based representations, while the other uses more associative and analogy-based reasoning or “language-based” common sense — the latter of which draws conclusions that are fuzzier but closer to the way that natural language works.
Whether you realize it or not, you interact with both of these kinds of systems on a daily basis.
You’ve probably heard of IBM’s Watson, which famously won at Jeopardy, but it’s a lesser-known fact that Watson’s predecessor was a project called Cyc that was developed in 1984 by Doug Lenat. The makers of Cyc, called Cycorp, operate a large repository of logic-based common sense facts. It’s still active today and remains one of the largest logic-based common sense projects.
In the school of language-based common sense, the Open Mind Common Sense project was started in 1999 by Marvin Minsky, Push Singh, and myself. OMCS and ConceptNet, its more well-known offshoot, include an information store in plain text, as well as a large knowledge graph. The project became an early success in crowdsourcing, and now ConceptNet contains 17 million facts in many languages.
Why Is It Important Now?
The last few years have seen great steps forward in particular types of machine learning: vector-based machine learning and deep learning. They have been instrumental in advancing language-based common sense, thus bringing computers one step closer to processing language the way humans do.
NLP is where common-sense reasoning excels, and the technology is starting to find its way into commercial products. Though there is still a long way to go, common-sense reasoning will continue to evolve rapidly in the coming years and the technology is stable enough to be in business use today. It holds significant advantages over existing ontology and rule-based systems, or systems based simply on machine learning.
It won’t be long before you have a more common-sense conversation with your computer about your trip to Mexico. And when you tell it that the water was a bit cold, your computer could reply: “I’m sorry to hear the ocean was chilly, it tends to be at this time of year. Though I saw the photos from your trip and it looks like you got to wear that lovely new bathing suit you bought last week.
A custom computer for custom computing is a technological extension of the physical user. How do you want to define yourself? The line in the sand has been drawn between REAL PC users, and brand-name “off-the-shelf” sellouts.
A lot of high-performance computer users choose to order a custom computer rather than buying a generic one. Apart from the obvious thrill and excitement of choosing your own parts based on your specific needs, there are many other reasons why computers are custom built. There are many prominent advantages of choosing custom computers over branded PCs.
Budget is one of the major motivating factors when it comes to choosing between branded and custom computers. There is a significant difference in cost between purchasing a readily available PC and building one yourself. Pre-built, branded PCs are priced to include many aspects like advertising costs, shipping costs, and just the hype associated with an established brand. They are highly overpriced, which means it is possible to save hundreds of dollars if you choose your own parts and we assemble them.
Our team takes care to ensure that the computers you purchase come with a warranty. How many of us know that the warranty is void if you even touch or just open up any of the parts? There are many ways in which companies can convince you that the machine is not eligible for warranty, such as saying that it has been meddled with. In case of a custom-built computer, you completely negate this possibility. Each and every part that goes into its construction comes with its own individual warranty. This means you get a separate warranty for the hard drive, the memory, the processor, and even for the case. If even a single part is malfunctioning, all you have to do is to remove it and send it for replacement. Warranties of parts cannot become void and you can be sure you will get what you are due in any scenario.
These are some of the obvious advantages of custom ordering your computer. It is not a very difficult proposition, either. If you are doing it for the first time, you can get someone’s help. We make sure to discover your exact needs and preferences so we can build you the computer that fits your needs. Furthermore, we grow as your needs grow too, so you are always on top of the game.
Think about a custom product, what comes to mind? Is it a tricked-out sports car? Is it the house of your dreams? Maybe a pineapple and anchovy pizza from your favorite, nifty food service app on your smartphone? The term “custom” has multiple definitions and can be used contextually in a variety of situations. From a literal perspective, custom is an adjective describing any product that is special in some way, individually crafted for a specific user, function or system, different from a generic or off-the-shelf. As a 21st century consumer, we should have the ability to pay for exactly what we want. If we desire a particular functionality, we have the ability to make those decisions, change the design, and better adhere to the particular situation.
To endow computers with common sense is one of the major long-term goals of Artificial Intelligence research. One approach to this problem is to formalize commonsense reasoning using mathematical logic. Commonsense Reasoning is a detailed, high-level reference on logic-based commonsense reasoning. It uses the event calculus, a highly powerful and usable tool for common sense reasoning, which Erik T. Mueller demonstrates as the most effective tool for the broadest range of applications. He provides an up-to-date work promoting the use of the event calculus for common sense reasoning, and bringing into one place information scattered across many books and papers. Mueller shares the knowledge gained in using the event calculus and extends the literature with detailed event calculus solutions to problems that span many areas of the commonsense world.
Custom Computing Custom computers are special-purpose systems customised for specific applications such as signal processing and database operations, when general-purpose computers are too slow, too bulky or consume too much power. Traditionally the development of custom computers is an expensive, time-consuming and error-prone activity. This course introduces an approach which enables the rapid and systematic design of custom computers.
Knowledge Discovery in Databases brings together current research on the exciting problem of discovering useful and interesting knowledge in databases. It spans many different approaches to discovery, including inductive learning, bayesian statistics, semantic query optimization, knowledge acquisition for expert systems, information theory, and fuzzy 1 sets.
The rapid growth in the number and size of databases creates a need for tools and techniques for intelligent data understanding. Relationships and patterns in data may enable a manufacturer to discover the cause of a persistent disk failure or the reason for consumer complaints. But today's databases hide their secrets beneath a cover of overwhelming detail. The task of uncovering these secrets is called "discovery in databases." This loosely defined subfield of machine learning is concerned with discovery from large amounts of possible uncertain data. Its techniques range from statistics to the use of domain knowledge to control search.
Data in real-time databases has to be logically consistent as well as temporally
consistent. The latter arises from the need to preserve the temporal validity of data
items that re
ect the state of the environment that is being controlled by the system.
Some of the timing constraints on the transactions that process real-time data come
from this need. These constraints, in turn, necessitate time-cognizant transaction
processing so that transactions can be processed to meet their deadlines.
This paper explores the issues in real-time database systems and presents an overview
of the state of the art. After introducing the characteristics of data and transactions in
real-time databases, we discuss issues that relate to the processing of time-constrained
transactions. Specically, we examine dierent approaches to resolving contention over
data and processing resources. We also explore the problems of recovery, managing
I/O, and handling overloads. Real-time databases have the potential to trade o the
quality of the result of a query or a transaction for its timely processing. Quality can
be measured in terms of the completeness, accuracy, currency, and consistency of the
results. Several aspects of this tradeo are also considered.
Many real-world applications involve time-constrained access to data as well as access to data
that has temporal validity. For example, consider telephone switching systems, network
management, program stock trading, managing automated factories, and command and
control systems. More specically, consider the following activities within these applications:
looking up the \800 directory", radar tracking and recognition of ob jects and determining
appropriate response, as well as the automatic tracking and directing of ob jects on a factory

oor. All of these involve gathering data from the environment, processing of gathered
information in the context of information acquired in the past, and providing timely response.
Another aspect of these examples is that they involve processing both temporal data, which
loses its validity after a certain interval, as well as archival data.
For instance, consider recognizing and directing ob jects moving along a set of conveyor
belts on a factory 
oor. An ob ject's features are captured by a camera to determine its type
and to recognize whether it has any abnormalities. Depending on the observed features, the
ob ject is directed to the appropriate workcell. In addition, the system updates its database
with information about the ob ject. The following aspects of this example are noteworthy.
First of all, features of an ob ject must be collected while the ob ject is still in front of the
camera. The collected features apply just to the ob ject in front of the camera, i.e., they lose
their validity once a dierent ob ject enters the system. Then the ob ject must be recognized
by matching the features against models for dierent ob jects stored in a database. This
matching has to be completed in time so that the command to direct the ob ject to the
appropriate destination can be given before the ob ject reaches the point where it must be
directed onto a dierent conveyor belt that will carry it to its next workcell. The database
update must also be completed in time so that the system's attention can move to the next
ob ject to be recognized. If, for any reason, a time-constrained actions is not completed
within the time limits, alternatives may be possible. In this example, if feature extraction
is not completed in time, the ob ject could be discarded for now to be brought back in front
of the camera at a later point in time. Applications such as these introduce the need for
real-time database systems.
During the last few years, the area of real-time databases has attracted the attention of
researchers in both real-time systems and database systems. The motivation of the database
researchers has been to bring to bear many of the benets of database technology to solve
problems in managing the data in real-time systems. Real-time system researchers have
been attracted by the opportunity real-time database systems provide to apply time-driven
scheduling and resource allocation algorithms. However, as we shall see, a simple integration
1
of concepts, mechanisms, and tools from database systems with those from real-time systems
is not feasible. Even a cursory examination of the characteristics of database systems and the
requirements of real-time systems will point out the various forms of \impedance mismatch"
that exist between them. Our goal in this paper is to point out the special characteristics, in
particular the temporal consistency requirements, of data in real-time databases, and show
how these lead to the imposition of time constraints on transaction execution. Meeting these
timing constraints demands new approaches to data and transaction management some of
which can be derived by tailoring, adapting, and extending solutions proposed for real-time
systems and database systems. Hence, as we present the issues in real-time database systems,
we review recent attempts at developing possible approaches to addressing these issues.
This paper is divided into roughly three parts. The rst part, corresponding to Sections
2, 3, and 4, introduces real-time database systems. Section 2 discusses the characteristics of
data in real-time database systems while Section 3 presents the characteristics of transactions
in real-time database systems. Many of these remind us of active databases. Hence Section
4 is devoted to an examination of the relationship between active databases and real-time
databases to point out the additional features we need in active databases in order to make
them suitable for use in a real-time database context.
The second part of the paper, contained in Section 5, discusses transaction processing
in real-time database systems. We review recent research in this area and show the need
to capitalize on, but adapt, current techniques from both real-time systems and database
systems.
The third part of the paper, contained in Section 6, discusses a number of issues in realtime databases some of which have seen little or no research. These include techniques to
trade o timeliness for quality, recovery of real-time transactions, and managing resources
other than CPU and data. Section 7 summarizes the paper.
In the rest of this introduction, we examine those characteristics of databases and realtime systems that are relevant to real-time database systems. We also point out the advantages of using databases to deal with data in real-time systems.
1.1 Databases and Real-Time Systems
Traditional databases, hereafter referred to simply as databases, deal with persistent data.
Transactions access this data while maintaining its consistency. Serializability is the usual
correctness criterion associated with transactions. The goal of transaction and query processing approaches adopted in databases is to achieve a good throughput or response time.
2
In contrast, real-time systems, for the most part, deal with temporal data, i.e., data that
becomes outdated after a certain time. Due to the temporal nature of the data and the
response time requirements imposed by the environment, tasks in real-time systems possess
time constraints, e.g., periods or deadlines. The resulting important dierence is that the
goal of real-time systems is to meet the time constraints of the activities.
One of the key points to remember here is that real-time does not just imply fast. Recall
the story of the tortoise and the hare. The hare was fast but was \busy" doing the wrong
activity at the wrong time. Even though we would like real-time systems to be faster than
the tortoise, we do require them to possess its predictability. Also, real-time does not imply
timing constraints that are in nanoseconds or seconds. For our purposes, real-time implies
the need to handle explicit time constraints, that is, to use time-cognizant protocols to deal
with deadlines or periodicity constraints associated with activities.
1.2 Why Real-Time Databases?
Databases combine several features that facilitate (1) the description of data, (2) the maintenance of correctness and integrity of the data, (3) ecient access to the data, and (4) the
correct executions of query and transaction executions in spite of concurrency and failures.
Specically,
 database schemas help avoid redundancy of data as well as of its description,
 data management support, such as indexing, assists in ecient access to the data, and
 transaction support, where transactions have ACID (Atomicity, Consistency, Isolation,
and Durability) properties, ensures correctness of concurrent transaction executions
and ensure data integrity maintenance even in the presence of failures.
However, support for real-time database systems must take into account the following.
Firstly, not all data in a real-time database are permanent; some are temporal. Secondly,
temporally-correct serializable schedules are a subset of the serializable schedules. Thirdly,
since timeliness is more important than correctness, in many situations, (approximate) correctness can be traded for timeliness. Similarly, atomicity may be relaxed. For instance, this
happens with monotonic queries and transactions, which are the counterparts of monotonic
tasks [35] in real-time systems. Furthermore, many of the extensions to serializability that
have been proposed in databases are also applicable to real-time databases (See [41] for a
review of these proposals). Some of these assume that isolation of transactions may not
always be needed. despite of these diferences, given the many advantages of database technology, it will
be benefcial if we can make use of them for managing data found in real-time systems. In
a similar vein, the advances made in real-time systems to process activities in time could be
exploited to deal with time-constrained transactions in real-time database systems.
As illustrated by the examples cited at the beginning of this section, many real-time
applications function in environments that are inherently distributed. Furthermore, many
real-time systems employ parallel processing elements for enhanced performance. Hence
parallel and distributed architectures are ubiquitous in real-time applications and hence
real-time database systems must be able to function in the context of such architectures.
The above discussion indicates that while many of the techniques used in real-time systems on the one hand, and databases systems on the other hand, may be applicable to
real-time database systems, many crucial dierences exist which either necessitate fresh approaches to some of the problems or require adaptations of approaches used in the two areas.
In the rest of the paper we will be substantiating this claim.
Characteristics of Data in Real-Time Database Systems
Typically, a realtime system consists of a a controlling system and a controlled system. For
example, in an automated factory, the controlled system is the factory oor with its robots,
assembling stations, and the assembled parts, while the controlling system is the computer
and human interfaces that manage and coordinate the activities on the factory. Thus,
the controlled system can be viewed as the environment with which the computer interacts.
The controlling system interacts with its environment based on the data available about
the environment, say from various sensors, e.g. temperature and pressure sensors. It is
imperative that the state of the environment, as perceived by the controlling system, be
consistent with the actual state of the environment. Otherwise, the eects of the controlling
systems' activities may be disastrous. Hence, timely monitoring of the environment as well as
timely processing of the sensed information is necessary. The sensed data is processed further
to derive new data. For example, the temperature and pressure information pertaining to
a reaction may be used to derive the rate at which the reaction appears to be progressing.
This derivation typically would depend on past temperature and pressure trends and so
some of the needed information may have to be fetched from archival storage (a temporal
database [46]). Based on the derived data, where the derivation may involve multiple steps,
actuator commands are set. For instance, in our example, the derived reaction rate is used
to determine the amount of chemicals or coolant to be added to the reaction. In general,
the history of (interactions with) the environment are also logged in archival storage.
In addition to the timing constraints that arise from the need to continuously track the
environment, timing correctness requirements in a real{time (database) system also arise
because of the need to make data available to the controlling system for its decision-making
activities. For example, if the computer controlling a robot does not command it to stop or
turn on time, the robot might collide with another ob ject on the factory  oor. Needless tosay, such a mishap can result in a ma jor catastrophe.
The need to maintain consistency between the actual state of the environment and the
state as reected by the contents of the database leads to the notion of temporal consistency.
Temporal consistency has two components [48, 4]:
Absolute consistency { between the state of the environment and its
Relationship to Active Databases
Many of the characteristics of data and transactions discussed in the last two sections may
remind a reader of active databases. Hence this section is devoted to a discussion of the
specific distinctions between active databases and real-time databases.
The basic building block in active databases is the following:
ON event
IF condition
DO action.
Upon the occurrence of the specified event, if the condition holds, then the specied action
can be taken. This construct provides a good mechanism by which integrity constraints
can be maintained among related or overlapping data or by which views can be constructed
[13]. The event can be arbitrary, including external events (as in the case of real-time events
generated by the environment), timer events, or transaction related events (such as the begin
and commit of transactions). The condition can correspond to conditions on the state of
the data or the environment. The action is said to be triggered [33, 12] and it can be an
arbitrary transaction.
Given this, it is not difficult to see that active databases provide a good model for
the arrival (i.e., triggering) of periodic/aperiodic activities based on events and conditions.
Even though the above construct implies that an active database can be made to react
to timeouts, time constraints are not explicitly considered by the underlying transaction
As mentioned earlier, this arises from the need to keep the controlling system's view
of the state of the environment consistent with the actual state of the environment.
Relative consistency { among the data used to derive other data.
This arises from the need to produce the sources of derived data close to each other. If a hard real-time transaction misses its deadline, it has catastrophic consequences. We can
also say that missing the deadline has a large negative value to the system. Thus, we would
like to predict beforehand that such transactions will complete before their deadlines. This
prediction will be possible only if we know the worst-case execution time of a transaction
and the data and resource needs of the transaction. In addition, it is desirable to have small
variance between the worst-case predictions and the actual needs. Predictability is also
important for soft deadline transactions, albeit to a lesser extent. In these cases, knowing
before a transaction begins that the transaction may not complete within its deadline allows
the system to discard the transaction, so that no time is spent on the transaction and no
recovery overheads are incurred The idea of programmability is the basis for the most precise definition of what SDN is: technology that separates the control plane management of network devices from the underlying data plane that forwards network traffic.
IDC broadens that definition of SDN in computer network by stating: “Datacenter SDN architectures feature software-defined overlays or controllers that are abstracted from the underlying network hardware, offering intent-or policy-based management of the network as a whole. This results in a datacenter network that is better aligned with the needs of application workloads through automated (thereby faster) provisioning, programmatic network management, pervasive application-oriented visibility, and where needed, direct integration with cloud orchestration platforms.”
The driving ideas behind the development of SDN are myriad. For example, it promises to reduce the complexity of statically defined networks; make automating network functions much easier; and allow for simpler provisioning and management of networked resources, everywhere from the data center to the campus or wide area network.
Separating the control and data planes is the most common way to think of what SDN in computer network is, but it is much more than that, said Mike Capuano, chief marketing officer for Pluribus.
“At its heart SDN has a centralized or distributed intelligent entity that has an entire view of the network, that can make routing and switching decisions based on that view,” Capuano said. “Typically, network routers and switches only know about their neighboring network gear. But with a properly configured SDN environment, that central entity can control everything, from easily changing policies to simplifying configuration and automation across the enterprise.” A variety of networking trends have played into the central idea of SDN. Distributing computing power to remote sites, moving data center functions to the edge, adopting cloud computing, and supporting Internet of Things environments – each of these efforts can be made easier and more cost efficient via a properly configured SDN environment.  
Typically in an SDN in computer network environment, customers can see all of their devices and TCP flows, which means they can slice up the network from the data or management plane to support a variety of applications and configurations, Capuano said. So users can more easily segment an IoT application from the production world if they want, for example. 
Some SDN in computer network controllers have the smarts to see that the network is getting congested and, in response, pump up bandwidth or processing to make sure remote and edge components don’t suffer latency.
SDN technologies also help in distributed locations that have few IT personnel on site, such as an enterprise branch office or service provider central office, said Michael Bushong, vice president of enterprise and cloud marketing at Juniper Networks.
“Naturally these places require remote and centralized delivery of connectivity, visibility and security. SDN solutions that centralize and abstract control and automate workflows across many places in the network, and their devices, improve operational reliability, speed and experience,” Bushong said. 
Tool breakage causes losses of surface polishing and dimensional accuracy for machined part, or possible damage to a workpiece or machine. Tool Condition Monitoring (TCM) is considerably vital in the manufacturing industry. In this paper, an indirect TCM approach is introduced with a wireless triaxial accelerometer. The vibrations in the three vertical directions (x, y and z) are acquired during milling operations, and the raw signals are de-noised by wavelet analysis. These features of de-noised signals are extracted in the time, frequency and time–frequency domains. The key features are selected based on Pearson’s Correlation Coefficient (PCC). The Neuro-Fuzzy Network (NFN) is adopted to predict the tool wear and Remaining Useful Life (RUL). In comparison with Back Propagation Neural Network (BPNN) and Radial Basis Function Network (RBFN), the results show that the NFN has the best performance in the prediction of tool wear and RUL.
Keywords: tool condition monitoring (TCM), remaining useful life (RUL), wireless sensor, wavelet analysis, wavelet packet transform (WPT), neuro-fuzzy network (NFN)
Traditional machining operations contain turning, milling, grinding and drilling, which are the most common operations in the manufacturing industry [1]. While the workpiece is machined, the contact between the cutter and workpiece causes the cutter shape to change, either tool wear gradually or tool breakage suddenly [2]. The problem of machine downtime continues to plague the manufacturing industry. Some sources that lead to downtime are unavoidable, for example, a workpiece is often transmitted from one workstation to another, which needs the time of dismantling and installation. In addition, a machine needs scheduled maintenance to ensure its normal operation. However, there are other downtime sources that could be avoidable, for example, the downtime caused by the tool wear or tool breakage. The failure of machine tools can attribute up to 20% of machine downtime [3]. Even if the tool does not be broken, the use of blunt tool or damaged cutter can result in extra strain on the processing equipment and quality loss in the machined workpiece. The purpose of Tool Condition Monitoring (TCM) is to adopt corresponding sensor signal processing techniques to monitor and predict the cutter state, in order to reduce the losses due to tool wear or tool damage. A powerful TCM system can improve productivity and guarantee product quality, which has a considerable influence on machining efficiency. Hence, TCM is considerably important in the manufacturing industry.
The measuring methods of tool wear are classified as direct (intermittent, offline) and indirect (continuous, online) methods based on monitoring period of signal acquisition. In direct measuring methods, such as tool-workpiece junction resistance, radioactivity, vision inspection, and optical and laser beams, the shape parameters of the cutter are measured by microscope, surface profiler, etc. [3]. The direct measuring methods have advantages of acquiring accurate dimension changes due to tool wear. However, these methods are vulnerable to field conditions, cutting fluid and various disturbances and are usually performed offline, and interrupts normal machining operations because of the contact between the tool and the measuring device, which severely limits the application of direct measuring methods. In indirect measuring methods, the tool wear is achieved by the corresponding sensor signals [4]. The measuring accuracy is lower than that of the direct measuring methods. However, they have the advantages of easy installation and easy to implement online in real time. This study focuses on indirect methods.
In the indirect methods, tool wear is measured based on various sensor signals containing cutting force, torque, vibration, Acoustic Emission (AE), sound, surface roughness, temperature, displacement, spindle power and current. Among these sensors, cutting force, vibration and AE measurements are robust and have been used more frequently than any other sensor measurement methods, and are more fit for the industrial field environment [5]. The features of the signals correlating to the tool wear are captured to monitor tool condition. To do this, a mass of signal processing methods were used, such as time series modeling, Fast Fourier Transform (FFT) and time–frequency analysis, the amount of calculation involved in corresponding parameters with tool wear is enormous. Wavelet Transform (WT) is a well-developed signal processing method and has been successfully used in various science and engineering fields. In the process of TCM, the sensor signals contain information and noise typically. Therefore, it is needed to de-noise and extract the features that contain the characteristics of the tool wear from various noise disturbances [6].
Generally, a Tool condition monitoring system consists of hardware and software parts to perform signal acquisition, signal preprocessing, features extraction, features selection and decision making [5]. A TCM system framework is presented in Figure 1. Signal acquisition belongs to the hardware part, but the remainder of the process including signal analysis, tool condition monitoring and Remaining Useful Life (RUL) prognostic belong to the software part. A reliable TCM system can prevent the occurrence of downtime and optimize the tool utilization in modern workpiece machining. TCM and RUL prognostics play an important role in modern manufacturing industry.
In information system and information technology, trust management is an abstract system that processes symbolic representations of social trust, usually to aid automated decision-making process. Such representations, e.g. in a form of cryptographic credentials, can link the abstract system of trust management with results of trust assessment. Trust management is popular in implementing information security, specifically access control policies.
The concept of trust management has been introduced by Matt Blaze[1] to aid the automated verification of actions against security policies. In this concept, actions are allowed if they demonstrate sufficient credentials, irrespective of their actual identity, separating symbolic representation of trust from the actual person.
Trust management can be best illustrated through the everyday experience of tickets. One can buy a ticket that entitles him e.g. to enter the stadium. The ticket acts as a symbol of trust, stating that the bearer of the ticket has paid for his seat and is entitled to enter. However, once bought, the ticket can be transferred to someone else, thus transferring such trust in a symbolic way. At the gate, only the ticket will be checked, not the identity of a bearer.
Trust management can be seen as a symbol-based automation of social decisions related to trust,[2] where social agents instruct their technical representations how to act while meeting technical representations of other agents. Further automation of this process can lead to automated trust negotiations (e.g. see Winslett[3]) where technical devices negotiate trust by selectively disclosing credential, according to rules defined by social agents that they represent. The definition and perspective on trust management was expanded in 2000 to include concepts of honesty, truthfulness, competence and reliability.[4] Trust levels, the nature of the trust relationship and the context were presented in the paper by Grandison and Sloman.
Web Services Trust Language (WS-Trust)[5] brings trust management into the environment of web services. The core proposition remain generally unchanged: the Web Service (verifier) is accepting a request only if the request contains proofs of claims (credentials) that satisfy the policy of a Web Service.
It is also possible to let technical agents monitor each other's behaviour and respond accordingly by increasing or decreasing trust. Such systems are collectively called Trust-Based Access Control (TBAC)[6] and their applicability have been studied for several different application areas.[7]
An alternative view on trust management[8] questions the possibility to technically manage trust, and focuses on supporting the proper assessment of the extent of trust one person has in the other.
Trust management is also studied in specific IT-related field such as transportation.[9]
Trust management is an important topic in online social network these days.
Machine-learning algorithms use statistics to find patterns in massive* amounts of data. And data, here, encompasses a lot of things—numbers, words, images, clicks, what have you. If it can be digitally stored, it can be fed into a machine-learning algorithm.
Machine learning is the process that powers many of the services we use today—recommendation systems like those on Netflix, YouTube, and Spotify; search engines like Google and Baidu; social-media feeds like Facebook and Twitter; voice assistants like Siri and Alexa. The list goes on.
In all of these instances, each platform is collecting as much data about you as possible—what genres you like watching, what links you are clicking, which statuses you are reacting to—and using machine learning to make a highly educated guess about what you might want next. Or, in the case of a voice assistant, about which words match best with the funny sounds coming out of your mouth.
Frankly, this process is quite basic: find the pattern, apply the pattern. But it pretty much runs the world. That’s in big part thanks to an invention in 1986, courtesy of Geoffrey Hinton, today known as the father of deep learning.
What is deep learning?
Deep learning is machine learning on steroids: it uses a technique that gives machines an enhanced ability to find—and amplify—even the smallest patterns. This technique is called a deep neural network—deep because it has many, many layers of simple computational nodes that work together to munch through data and deliver a final result in the form of the prediction.
In unsupervised learning, the data has no labels. The machine just looks for whatever patterns it can find. This is like letting a dog smell tons of different objects and sorting them into groups with similar smells. Unsupervised techniques aren’t as popular because they have less obvious applications. Interestingly, they have gained traction in cybersecurity.
What is reinforcement learning?
Lastly, we have reinforcement learning, the latest frontier of machine learning. A reinforcement algorithm learns by trial and error to achieve a clear objective. It tries out lots of different things and is rewarded or penalized depending on whether its behaviors help or hinder it from reaching its objective. This is like giving and withholding treats when teaching a dog a new trick. Reinforcement learning is the basis of Google’s AlphaGo, the program that famously beat the best human players in the complex game of Go.
Discrete Math is the language of Computer Science. One needs to be fluent in it to work in many fields including data science, machine learning, and software engineering (it is not a coincidence that math puzzles are often used for interviews). We introduce you to this language through a fun try-this-before-we-explain-everything approach: first you solve many interactive puzzles that are carefully designed specifically for this online specialization, and then we explain how to solve the puzzles, and introduce important ideas along the way. We believe that this way, you will get a deeper understanding and will better appreciate the beauty of the underlying ideas (not to mention the self confidence that you gain if you invent these ideas on your own!). To bring your experience closer to IT-applications, we incorporate programming examples, problems, and projects in the specialization.
Data management is an administrative process that includes acquiring, validating, storing, protecting, and processing required data to ensure the accessibility, reliability, and timeliness of the data for its users. Organizations and enterprises are making use of Big Data more than ever before to inform business decisions and gain deep insights into customer behavior, trends, and opportunities for creating extraordinary customer experiences.
what is data managementTo make sense of the vast quantities of data that enterprises are gathering, analyzing, and storing today, companies turn to data management solutions and platforms. Data management solutions make processing, validation, and other essential functions simpler and less time-intensive.
Leading data management platforms allow enterprises to leverage Big Data from all data sources, in real-time, to allow for more effective engagement with customers, and for increased customer lifetime value (CLV). Data management software is essential, as we are creating and consuming data at unprecedented rates. Top data management platforms give enterprises and organizations a 360-degree view of their customers and the complete visibility needed to gain deep, critical insights into consumer behavior that give brands a competitive edge.
Data Management Challenges
While some companies are good at collecting data, they are not managing it well enough to make sense of it. Simply collecting data is not enough; enterprises and organizations need to understand from the start that data management and data analytics only will be successful when they first put some thought into how they will gain value from their raw data. They can then move beyond raw data collection with efficient systems for processing, storing, and validating data, as well as effective analysis strategies.
Another challenge of data management occurs when companies categorize data and organize it without first considering the answers they hope to glean from the data. Each step of data collection and management must lead toward acquiring the right data and analyzing it in order to get the actionable intelligence necessary for making truly data-driven business decisions.
Data Management Best Practices
The best way to manage data, and eventually get the insights needed to make data-driven decisions, is to begin with a business question and acquire the data that is needed to answer that question. Companies must collect vast amounts of information from various sources and then utilize best practices while going through the process of storing and managing the data, cleaning and mining the data, and then analyzing and visualizing the data in order to inform their business decisions.
It’s important to keep in mind that data management best practices result in better analytics. By correctly managing and preparing the data for analytics, companies optimize their Big Data. A few data management best practices organizations and enterprises should strive to achieve include:
Simplify access to traditional and emerging data
Scrub data to infuse quality into existing business processes
Shape data using flexible manipulation techniques
It is with the help of data management platforms that organizations have the ability to gather, sort, and house their information and then repackage it in visualized ways that are useful to marketers. Top performing data management platforms are capable of managing all of the data from all data sources in a central location, giving marketers and executives the most accurate business and customer information available.
Benefits of Data Management and Data Management Platforms
data management platformManaging your data is the first step toward handling the large volume of data, both structured and unstructured, that floods businesses daily. It is only through data management best practices that organizations are able to harness the power of their data and gain the insights they need to make the data useful.
Looking to learn new skills or brush up some good old skills while the quarantine drives to an end?
The impact of this global pandemic and economic slowdown on the job market is very uneven. While remote work enablers like Slack and Zoom are accelerating recruitments, we must address the layoffs and positions being cancelled. In such a time, we all want to put our best feet forward for the positions that are still open.
It’s a known fact that the biggest battle to be fought is with the ATS. Every Data Science position has roles and responsibilities a hired would adhere to. If the ATS is unable to identify any of the important skills or keywords, thinking you’re not fit for a job from your resume, then you might not pass the ATS readability test.
To pass through the ATS with flying colors and highlight your skills, I have researched a list of top 30 skills and keywords for your next Data Science role in alphabetical order. Let’s see what they are.
1. AWS
Cloud services are useful to businesses of all sizes to prepare the backend infrastructure, decrease cloud storage expenses significantly by paying only when needed. AWS, a pioneer in cloud computing has been a vibrant tool for Data Scientists.
Elastic Compute Cloud (EC2)
Simple Storage Service (S3)
Relational Database Service (RDS)
RedShift
Elastic MapReduce (EMR)
are some of the instances used with AWS. Experience working with cloud services, especially a popular one like AWS, is a huge plus in your Data Science career.
Majority of the technology companies depend on these services now and use them unremittingly. Thus, if you are familiar with one of these services, it is sure to give them the confidence that you need less training to get on-board. With more and more people moving into data science, you want your resume to stand out as much as possible!
2. Big Data
Data Science is a field that comprises of everything related to data. From cleansing, mining, preparation, and analysis, Data Science is a process.
Big Data refers to the vast volume of data that is difficult to store and process in real-time. This data can be used to analyze insights which can lead to better decision making. The principles of Data Science remains the same, the data size multiplies in folds.
In a real business world, it is the Big Data that is worked on. So from now, whatever projects you do, Big Data is something you can eye on to analyse.
3. Business Intelligence (BI)
I am not saying Business Intelligence is a part of Data Science.
However, since both of them have to do a lot with examining data only to be used by business processes, BI is often sought with Data Science. In a nutshell, while BI helps interpret past data, Data Science can analyze the past data, identify trends or patterns to make future predictions.
BI is mainly used for reporting or Descriptive Analytics which can also be done with Data Science and therefore, it is a good skill to learn.
Popular BI tools:
PowerBI by Microsoft
Tableau
SAP Business Intelligence (for enterprise)
Qlik
4. Cloud Computing
The practice of data science often includes the use of cloud computing products and services to help data professionals access the resources needed to manage and process data.
An everyday role of a Data Scientist generally includes analyzing and visualizing data that are stored in the cloud. You may have read that data science and cloud computing go hand in hand, typically because Cloud computing gives a hand to data scientists to use platforms that provide access to databases, frameworks, programming languages, and operational tools.
Amazon — AWS
Google Cloud Platform — GCP
Microsoft — Azure
Alibaba — Alibaba Cloud
5. Data Analytics
Data science is an umbrella term that encompasses several related disciplines including Data Analytics. While a data scientist is expected to forecast the future based on past patterns, data analysts extract meaningful insights from various data sources.
Data Analytics is spread across descriptive, diagnostics, prescriptive, predictive analysis, each having its own application.
The applications of data analytics are seemingly endless. More and more data is being collected every day presenting new opportunities to apply data analytics to more parts of business, science and everyday life.
6. Data Exploration
You have data in the warehouse, but that data is pretty inconsistent. So you have to clean and unify the messy and complex data sets for easy access and analysis.
Exploratory Data Analysis (EDA) is the first step in your data analysis process. Here, you make sense of the data you have and then figure out what questions you want to ask and how to frame them, as well as how best to manipulate your available data sources to get the answers you need.
7. Data Management
A lot of data you will be working on will be messy, values could be missing, there could be inconsistent formatting with dates and strings. You will need to clean and wrangle your data before you start processing.
The value of data is not based on its source, quality or format; its value depends on what you do with it!
Data Management involves acquiring, validating, storing, protecting, and processing required data. Data Management can include skills like —
Data Wrangling — transform and map data for downstream operations
Data Processing — retrieve, transform, classify information from raw data
Data Security — protection from unauthorized access and data corruption
Data Governance — manage the availability, usability, integrity, & security
Data Manipulation — make data easier to read or more organized
Data Modelling — associations between data, querying, design data flow
Data Migration — select, prepare, extract, transform, transfer data
Data Warehousing — store data from sources for reporting and analysis
Data Transformation, and more..
8. Data Visualization
Data Visualization is one of the most important parts of data analysis. It has always been important to present the data in an understandable and visually appealing format. Data visualization is one of the skills that Data Scientists have to master in order to communicate better with the end users. There are multiple tools like Tableau, Power BI which gives you a nice intuitive interface.
This is an essential part of data science, of course, as it lets the scientist describe and communicate their findings to technical and non-technical audiences. Tools like matplotlib, ggplot, or d3.js let us do just that. Another good tool for this is Tableau.
9. DevOps
I’ve always heard and believed that Data Science is for someone who knows mathematics, statistics, algorithms, and data management. However, recently, I came across the growing significance of DevOps for Data Science.
DevOps is a set of methods that combines software development and IT operations that aims to shorten the development life cycle and provide uninterrupted delivery with high software quality.
DevOps teams closely work with the development teams to manage the lifecycle of applications effectively. Data transformation demands close collaboration of data science teams with DevOps. DevOps team is expected to provide highly available clusters of Apache Hadoop, Apache Kafka, Apache Spark, and Apache Airflow to tackle data extraction and transformation.
What can be done with DevOps for Data Science?
Provision, configure, scale and manage data clusters
Manage information infrastructure by continuous integration, deployment, and monitoring of data
Create scripts to automate the provisioning and configuration of the foundation for a variety of environments.
10. HADOOP
Hadoop is a must for Data Scientists.
The main functionality of Hadoop is storage of Big Data. It also allows the users to store all forms of data, that is, both structured data and unstructured data. Hadoop also provides modules like Pig and Hive for analysis of large scale data.
Now, I won’t say Hadoop is necessary to become a Data Scientist, but a data scientist must know how to get the data out in the first place to do analysis and Hadoop is exactly the technology that stores large volumes of data, where a data scientist can work on.
11. Machine Learning
Machine Learning, as the name suggests, is the process of making machines intelligent, that have the power to think, analyze and make decisions. By building precise Machine Learning models, an organization has a better chance of identifying profitable opportunities — or avoiding unknown risks.
You should have good hands-on knowledge of various Supervised and Unsupervised algorithms.
Deep Learning has taken traditional Machine Learning approaches to a next level. It is inspired by biological Neurons (Brain Cells). The idea here is to mimic the human brain. A large network of such Artificial Neurons is used, this is known as Deep Neural Networks. Nowadays, most of the organizations ask for knowledge of Deep Learning, so don’t miss this.
12. Multivariate Calculus & Linear Algebra
Most machine learning, invariably data science models, are built with several predictors or unknown variables. A knowledge of multivariate calculus is significant for building a machine learning model. Here are some of the topics of math you can be familiar with to work in Data Science:
Derivatives and gradients
Step function, Sigmoid function, Logit function, ReLU (Rectified Linear Unit) function
Cost function (most important)
Plotting of functions
Minimum and Maximum values of a function
Scalar, vector, matrix and tensor functions
Data science is probably not a good career choice for people who don’t like or are not proficient at mathematics. The data scientist whiz is one who excels at mathematics and statistics, while having an ability to collaborate closely with line-of-business executives to communicate what is actually happening in the “black box” of complex equations
13. MATLAB
MATLAB, developed by MathWorks displays a full set of capabilities for deep learning and provides end-to-end integrated workflow from research to prototype
Data Science and Machine learning have to do a lot with matrices and MATLAB works best for matrix calculations, design complex neural architectures with ease in fewer lines of code.
Andrew Ng’s course on ML on Coursera teaches Machine Learning on Octave — a synonym to MATLAB. An added skill wouldn’t hurt to make your resume notice-worthy, would it?
14. Python, R
Of course! Data Science essentially is about programming. Programming Skills for Data Science brings together all the fundamental skills needed to transform raw data into actionable insights. While there is no specific rule about the selection of programming language, Python and R are the most favored ones.
I’m not a religious person about programming language preferences or platforms. Data Scientists choose a programming language that serves the need of a problem statement in hand. Python, however, seems to have become the closest thing to a lingua franca for data science.
Read more about the Top 10 Python Libraries for Data Science here.
15. SAS
In terms of software for analytics, SAS is one of the oldest. SAS has its own programming language that resembles SQL. Future for any data analytics languages is really bright for the next some years as trained professionals are not available as per high demand. The main reasons to get into SAS are —
SAS jobs — SAS programming skills in demand are amazing to know. Statistics show that 70% of analytics jobs are in SAS Programming, followed by R and then Python. It ever-evolving features according to industry needs is one major factor in its favour.
There is a huge scope of SAS for a fresher. Banks are heavily using SAS as are Insurance & other Financial Services companies like HSBC, Citi, JP Morgan, & Wells Fargo. SAS offers mutiple certification programs to get yourself trained in the skill.
16. SPSS
The IBM SPSS software platform offers advanced statistical analysis, a vast library of machine learning algorithms, text analysis, open source extensibility, integration with big data and seamless deployment into applications.
IBM SPSS is a relatively lesser used tool, I can say. It is more popular among research scholars and academicians for research studies in the area of social sciences, psychology, finance, human resource etc. While most data scientists and industries use R, Python, or SAS as primary tools, SPSS is a great tool if you’re working in the area of psychology or related field.
Some also say that SPSS is dying with R taking over, SPSS
17. Statistics
As a data scientist, you should be capable of working with tools like statistical tests, distributions, and maximum likelihood estimators. A good data scientist will realize what technique is a valid approach to her/his problem. With statistics, you can help stakeholders take decisions and design and evaluate experiments.
18. SQL
SQL is a fourth-generation language; a domain-specific language designed to manage data stored in an RDMS (Relational Database Management System) and for steam processing in an RDSMS (Relational Data Stream Management System). We can use it to handle structured data in situations where variables of data relate to each other, which is core to Data Science
19. Tableau
Tableau is a very popular and powerful data visualization tool used in Business Intelligence these days. Data analytics is rapid and robust with Tableau. With the visualization dashboards and worksheets, Tableau covers from simplifying raw data into a very easily understandable format, cleaning data and identifying underlying trends and patterns.
For quite some time now, big firms like PepsiCo, Verizon, Charles Schwab, Coca-Cola, Chipotle are using Tableau to leverage their data and make conclusive informed decisions. A tool like Tableau is a must-know for anyone interested in a Data Science or Data Analytics career.
20. VBA
Excel is one of the most commonly used tools in a wide variety of businesses. Excel on its own is very flexible and powerful, however, when even the complex functions aren’t good enough, VBA comes into the picture.
You can compute, simulate or workout the data in VBA. Excel VBA has its own limitations and alternatives. The same work can be done efficiently in SQL or Python as well, however, it clearly depends on your business requirements.
Note: You may want to learn VBA if you work in an environment where excel files take a heavy part in the workflow, and, particularly, where the output of your own work has to be an excel file itself.
Data scientists increasingly work across entire organizations, and communication skills are as important as technical ability. Data science is booming in every industry, as more people and companies are investing their time to better understand this constantly expanding field. The ability to communicate effectively is a key talent differentiator.
Whether you pursue a deeper knowledge of data science by learning a specialty, or simply want to gain a smart overview of the field, mastering the right terms will fast-track you to success on your educational and professional journey.
According to Vinod Bakthavachalam, a senior data scientist at Coursera, using the following data science terms accurately will help you stand out from the crowd:
Business Intelligence (BI). BI is the process of analyzing and reporting historical data to guide future decision-making. BI helps leaders make better strategic decisions moving forward by determining what happened in the past using data, like sales statistics and operational metrics.
Data Engineering. Data engineers build the infrastructure through which data is gathered, cleaned, stored and prepped for use by data scientists. Good engineers are invaluable, and building a data science team without them is a “cart before the horse” approach.
Decision Science. Under the umbrella of data science, decision scientists apply math and technology to solve business problems and add in behavioral science and design thinking (a process that aims to better understand the end user).
Artificial Intelligence (AI). AI computer systems can perform tasks that normally require human intelligence. This doesn’t necessarily mean replicating the human mind, but instead involves using human reasoning as a model to provide better services or create better products, such as speech recognition, decision-making and language translation.
Machine Learning. A subset of AI, machine learning refers to the process by which a system learns from inputted data by identifying patterns in that data, and then applying those patterns to new problems or requests. It allows data scientists to teach a computer to carry out tasks, rather than programming it to carry out each task step-by-step. It’s used, for example, to learn a consumer’s preferences and buying patterns to recommend products on Amazon or sift through resumes to identify the highest-potential job candidates based on key words and phrases.
 Supervised Learning. This is a specific type of machine learning that involves the data scientist acting as a guide to teach the desired conclusion to the algorithm. For instance, the computer learns to identify animals by being trained on a dataset of images that are properly labeled with each species and its characteristics.
Classification is an example of supervised learning in which an algorithm puts a new piece of data under a pre-existing category, based on a set of characteristics for which the category is already known. For example, it can be used to determine if a customer is likely to spend over $20 online, based on their similarity to other customers who have previously spent that amount.
Cross validation is a method to validate the stability, or accuracy, of your machine-learning model. Although there are several types of cross validation, the most basic one involves splitting your training set in two and training the algorithm on one subset before applying it the second subset. Because you know what output you should receive, you can assess a model’s validity.
Clustering is classification but without the supervised learning aspect. With clustering, the algorithm receives inputted data and finds similarities in the data itself by grouping data points together that are alike.
Deep Learning. A more advanced form of machine learning, deep learning refers to systems with multiple input/output layers, as opposed to shallow systems with one input/output layer. In deep learning, there are several rounds of data input/output required to assist computers to solve complex, real-world problems. A deep dive can be found here.
Linear Regression. Linear regression models the relationship between two variables by fitting a linear equation to the observed data. By doing so, you can predict an unknown variable based on its related known variable. A simple example is the relationship between an individual’s height and weight.
A/B Testing. Generally used in product development, A/B testing is a randomized experiment in which you test two variants to determine the best course of action. For example, Google famously tested various shades of blue to determine which shade earned the most clicks.
Hypothesis Testing. Hypothesis testing is the use of statistics to determine the probability that a given hypothesis is true. It’s frequently used in clinical research.
Statistical Power. Statistical power is the probability of making the correct decision to reject the null hypothesis when the null hypothesis is false. In other words, it’s the likelihood a study will detect an effect when there is an effect to be detected. A high statistical power means a lower likelihood of concluding incorrectly that a variable has no effect.
Standard Error. Standard error is the measure of the statistical accuracy of an estimate. A larger sample size decreases the standard error.
Causal inference is a process that tests whether there is a relationship between cause and effect in a given situation—the goal of many data analyses in social and health sciences. They typically require not only good data and algorithms, but also subject-matter expertise.
Exploratory Data Analysis (EDA). EDA is often the first step when analyzing datasets. With EDA techniques, data scientists can summarize a dataset’s main characteristics and inform the development of more complex models or logical next steps.
Data Visualization. A key component of data science, data visualizations are the visual representations of text-based information to better detect and recognize patterns, trends and correlations. It helps people understand the significance of data by placing it in a visual context.
R. R is a programming language and software environment for statistical computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.
Python is a programming language for general-purpose programming and is one language used to manipulate and store data. Many highly trafficked websites, such as YouTube, are created using Python.
SQL. Structured Query Language, or SQL, is another programming language that is used to perform tasks, such as updating or retrieving data for a database.
ETL. ETL is a type of data integration that refers to the three steps (extract, transform, load) used to blend data from multiple sources. It’s often deployed to build a data warehouse. An important aspect of this data warehousing is that it consolidates data from multiple sources and transforms it into a common, useful format. For example, ETL normalizes data from multiple business departments and processes to make it standardized and consistent.
GitHub. GitHub is a code-sharing and publishing service, as well as a community for developers. It provides access control and several collaboration features, such as bug tracking, feature requests, task management and wikis for every project. GitHub offers both private repositories and free accounts, which are commonly used to host open-source software projects.
Data Models define how datasets are connected to each other and how they are processed and stored inside a system. Data models show the structure of a database, including the relationships and constraints, which helps data scientists understand how the data can best be stored and manipulated.
Data Warehouse. A data warehouse is a repository where all the data collected by an organization is stored and used as a guide to make management decisions.
What happens now is the top, or system, level definition of the system.
Tasks will be created and the various functions will be assigned to them. A communications plan will be developed to handle data transfers between the tasks. A system timing analysis will be performed to determine the system timing tick.
The system modes and priorities will be analyzed, and a system-level error detection and handling system will be defined. Basically, a complete high-level blueprint for the system will be generated, with module specifications for each of the tasks and major systems in the design.
The first step in the system-level design is task definition. Task definition is the process of gathering the various software functions from the requirements document dissection together and grouping them into a minimal number of tasks. Each task will be a separate execution module, with its own specific timing, priority, and communications pathways.
Because of this, the functions within the module must be compatible, or at least capable of operating without interfering with one another. Now a typical question at this point is “Why a minimal number of tasks—why not create a task for every function?”
That would eliminate the need to determine whether or not the various functions are compatible. However, there are two main problems: overhead and synchronization. Overhead is the amount of additional code required to manage a function, the switch statement, the timing handler, and any input/output routines required for communications.
Synchronization is the need for some of the software functions to coordinate their function with other functions in the system. Placing compatible functions into a single task accomplishes both goals, the overhead for a group of functions is combined into a single task, and because the functions share a common task, they can coordinate activities without complex handshaking.
An example would be combining a cursor function and a display-scanning function into a common task. Putting the two functions together reduces the additional code by half, and it allows the designers to coordinate their activity by combining them into a single execution string.
In this context, there are valid reasons why some of the functions should be combined into a common task. This is not to say that all software functions should be combined into common tasks.
After all, the whole purpose of this design methodology is to generate software that can execute more than one task simultaneously. And there are very good reasons why some software functions are so incompatible that they can’t or shouldn’t be combined into a common task. Part of task definition is to analyze the various software functions and determine which, if any, functions should be combined.
So, how does a designer decide which functions are compatible and which are not? The simplest method is to start combining similar functions into tasks, and then determine if the combination is compatible.
To do this, start by writing the name of each function on a piece of tape. Drafting tape works best because it is designed to be stuck down and taken up repeatedly without much trouble.
Next, take a large piece of paper and draw 10–15 large circles on it, each about 5–8 inches in diameter. The placement of the circles is not critical; just distribute them evenly on the paper. Then take the strips of tape with the function names, and place them within the circle on the sheet of paper. Try to group like functions together, and try to limit the number of circles used.
Don’t worry at this point if some circles have more names inside than others do. We are just trying to generate a preliminary distribution of the functions
Once all the functions have been distributed into the circles on the paper, take a pencil (not a pen) and name the circles that have pieces of tape in them. Use a name that is generally descriptive of the collection of functions within the circle.
For example, if a circle contains several functions associated with interpreting and executing user commands, then COMMAND would be a good label. Try not to be too specific, as the exact mix of functions will most likely change over the course of the analysis for compatibility.
And don’t be concerned if all the functions are moved out of a specific circle. The names are just for convenience at this point. The final naming and grouping of functions will be decided at the end of the process.
An embedded system is a part of a product with which an end user does not directly interact or control. Products with embedded systems in them include modems, disk drives, digital cellular phones, radios, audio CD players, music synthesizers, video disk players, sonar, radar, confocal microscopes, Magnetic Resonance Imaging (MRI) medical systems, video telephones, and missiles. The fundamental problem facing the design of embedded systems is heterogeneity. Multiple styles of algorithms (e.g. signal processing, communications, and controls) are implemented using a variety of technologies (e.g., digital signal processors, microcontrollers, field-programmable gate arrays, application-specific integrated circuits, and real-time operating systems).
A common practice in industry is to use a style of specification that biases the possible implementation technology. Specifying an algorithm in C or VHDL would bias the implementation towards compiled code running on a programmable processor or hardware, respectively. It is not possible to convert all possible C programs into statically schedulable VHDL code because C is Turing equivalent, which means that C programs could have infinite state. Some specification languages, e.g. imperative programs such as C, force the designer to make several arbitrary choices about the sequential order of the subtasks. An inherent parallelism at a functional level is now hidden, and many good alternatives may be missed.
By decoupling the specification from implementation and using formal mathematical models of computation for specification, we gain the ability to perform fast simulation and efficient synthesis of complex heterogeneous systems. We model complex systems as a hierarchical composition of the simpler models of computation. Some of these simpler models of computation, such as types of finite state machines, dataflow models, and synchronous/reactive models, have finite state. Because they have finite state, all analyses of the system can be performed at compile-time. For example, memory usage and execution time can be determined without having to run the system. These models can be overlaid on an implementation technology (such as C or VHDL).
In the first half of the talk, we will discuss several issues in system-level design including cosimulation and codesign. We will survey different models of computation and communication for control, signal processing, and communications applications. In the second half of the talk, we will first describe dataflow modeling. Then, we will focus on the Synchronous Dataflow model, which can be statically scheduled. The Synchronous Dataflow model is determinate: the state of the system (the set of signals on all of the arcs) is not affected by the scheduling algorithm. Using the Synchronous Dataflow, we guarantee at compile time that a graph (program) is consistent and will not deadlock.
A multi agent system (MAS or "self-organized system") is a computerized system composed of multiple interacting intelligent agents. Multi agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve.[1] Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.
espite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM). The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be "intelligent") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the science, and MAS in engineering and technology.[3] Applications where multi-agent systems research may deliver an appropriate approach include online trading,[4] disaster response[5][6] and social structure modelling.
pro-activeness: agents do not simply act in response to their environment, they are able to exhibit goal-directed behavior by taking the initiative.”
This definition does not specify the size of agents. They can be as big as expert system and as small as the part of an application interface. Agent can be static (permanently located in some computer) or mobile (moving across the computer network, such as Internet). The amount of agent intelligence is also not specified. A collection of agent definitions is given in [10].
Agent oriented software engineering programming languages are programming languages developed for programming of agents. Agent-oriented programming (AOP) can also be seen as a post-object-oriented paradigm.
An advantage of the usage of agents in software development instead of objects stems from the primitives used for programming. AOP introduces new concepts such as mental categories, reactivity, pro-activeness, concurrent execution inside and between agents, communication, meta-level reasoning, etc.
This paper presents an AOP language named LASS. Agent programmed with LASS possesses intentions, beliefs, and plans for its public and internal services. Besides deliberative properties, agent specified with LASS can behave reactively as well. LASS introduces the usage of behaviors - programming primitives enabling agent to react immediately when it is necessary [2]. LASS enables powerful communication between agents which is based on agent public services. Services are used similarly like remote procedure calls.
The Internet is a network of networks, and autonomous systems are the big networks that make up the Internet. More specifically, an autonomous system (AS) is a large network or group of networks that has a unified routing policy. Every computer or device that connects to the Internet is connected to an AS.
Imagine an AS as being like a town's post office. Mail goes from post office to post office until it reaches the right town, and that town's post office will then deliver the mail within that town. Similarly, data packets cross the Internet by hopping from AS to AS until they reach the AS that contains their destination Internet Protocol (IP) address. Routers within that AS send the packet to the IP address.
Every autonomous systems controls a specific set of IP addresses, just as every town's post office is responsible for delivering mail to all the addresses within that town. The range of IP addresses that a given AS has control over is called their "IP address space."
Most Autonomous systems connect to several other ASes. If an AS connects to only one other AS and shares the same routing policy, it may instead be considered a subnetwork of the first AS.
Typically, each AS is operated by a single large organization, such as an Internet service provider (ISP), a large enterprise technology company, a university, or a government agency.
Self-driving cars, collaborative production assistants, and socially-enabled domestic robots are examples of autonomous systems at Bosch. Autonomous systems operate in complex and open-ended environments with high levels of independence and self-determination. They perceive, learn, reason and act with self-awareness and respond intelligently to unforeseen changes in the environment. We believe that such systems have the potential to deeply impact 21st century societies. They will increase quality of life for everyone, transform mobility, raise productivity, improve resource use, and enhance human safety.
Robotics is both an enabling technology for autonomous systems and an application domain. We work on problems that extend the state of the art in simultaneous localization and mapping (SLAM), robot perception, planning, simulation and coordination and control, using both model and data-based methods. Examples include 24/7 year-round cm-accuracy life-long SLAM, long-term human motion prediction and deep reinforcement learning for manipulation. Our solutions are transferred into a uniquely diverse portfolio of products ranging from lightweight consumer robots, intralogistics transportation platforms, collaborative production assistants, and field robots for precision agriculture to 60-ton cargo movers.
Automated driving will be the cornerstone of future mobility systems. Highly automated and driverless cars will make traffic safer, more convenient and more efficient. Our research in automated driving spans a large variety of topics from deep learning for multi-modal scene perception and understanding, online decision-making, and large-scale simulations to novel methodologies for validation and safety. We additionally focus on training, testing, increasing robustness as well as generalization of our methods across different vehicles, use-cases, and business models. The outcomes affect partially and highly automated driving in fully automated and mixed traffic.
n the Internet, an autonomous system (AS) is the unit of router policy, either a single network or a group of networks that is controlled by a common network administrator (or group of administrators) on behalf of a single administrative entity (such as a university, a business enterprise, or a business division). An autonomous system is also sometimes referred to as a routing domain. An autonomous system is assigned a globally unique number, sometimes called an Autonomous System Number (ASN).
Reasoning is the ability to make inferences, and automated reasoning is concerned with the building of computing systems that automate this process. Although the overall goal is to mechanize different forms of reasoning, the term has largely been identified with valid deductive reasoning as practiced in mathematics and formal logic. In this respect, automated reasoning is akin to mechanical theorem proving. Building an automated reasoning program means providing an algorithmic description to a formal calculus so that it can be implemented on a computer to prove theorems of the calculus in an efficient manner. Important aspects of this exercise involve defining the class of problems the program will be required to solve, deciding what language will be used by the program to represent the information given to it as well as new information inferred by the program, specifying the mechanism that the program will use to conduct deductive inferences, and figuring out how to perform all these computations efficiently While basic research work continues in order to provide the necessary theoretical framework, the field has reached a point where automated reasoning programs are being used by researchers to attack open questions in mathematics and logic, provide important applications in computing science, solve problems in engineering, and find novel approaches to questions in exact philosophy.
A second important consideration in the building of an automated reasoning program is to decide (1) how problems in its domain will be presented to the reasoning program; (2) how they will actually be represented internally within the program; and, (3) how the solutions found—completed proofs—will be displayed back to the user. There are several formalisms available for this, and the choice is dependent on the problem domain and the underlying deduction calculus used by the reasoning program. The most commonly used formalisms include standard first-order logic, typed λ-calculus, and clausal logic. We take up clausal logic here and assume that the reader is familiar with the rudiments of first-order logic; for the typed λ-calculus the reader may want to check Church 1940. Clausal logic is a quantifier-free variation of first-order logic and has been the most widely used notation within the automated reasoning community. Some definitions are in order: A term is a constant, a variable, or a function whose arguments are themselves terms. For example, a, x, f(x), and h(c,f(z),y) are all terms. A literal is either an atomic formula, e.g. F(x), or the negation of an atomic formula, e.g. ~R(x,f(a)). Two literals are complementary if one is the negation of the other. A clause is a (possibly empty) finite disjunction of literals l1∨ … ∨ ln where no literal appears more than once in the clause (that is, clauses can be alternatively treated as sets of literals). Ground terms, ground literals, and ground clauses have no variables. The empty clause, [ ], is the clause having no literals and, hence, is unsatisfiable—false under any interpretation. Some examples: ~R(a,b), and F(a) ∨ ~R(f(x),b) ∨ F(z) are both examples of clauses but only the former is ground. The general idea is to be able to express a problem’s formulation as a set of clauses or, equivalently, as a formula in conjunctive normal form (CNF), that is, as a conjunction of clauses.
For formulas already expressed in standard logic notation, there is a systematic two-step procedure for transforming them into conjunctive normal form. The first step consists in re-expressing a formula into a semantically equivalent formula in prenex normal form, (Θx1)…(Θxn)α(x1,…,xn), consisting of a string of quantifiers (Θx1)…(Θxn) followed by a quantifier-free expression α(x1,…,xn) called the matrix. The second step in the transformation first converts the matrix into conjunctive normal form by using well-known logical equivalences such as DeMorgan’s laws, distribution, double-negation, and others; then, the quantifiers in front of the matrix, which is now in conjunctive normal form, are dropped according to certain rules. In the presence of existential quantifiers, this latter step does not always preserve equivalence and requires the introduction of Skolem functions whose role is to “simulate” the behaviour of existentially quantified variables. For example, applying the skolemizing process to the
Brain computer interfaces (BCIs) measure brain activity, extract features from that activity, and convert those features into outputs that replace, restore, enhance, supplement, or improve human functions.
Multimedia communication services (MMCS) or “media spaces” is a rapidly developing area of computer-based networking with major privacy implications. These systems, based on the use of video and audio connections between workplaces, have been caricatured as “electronic fishbowls” in which every act is visible, at least to those with access. Developed largely in the context of computer-supported cooperative work (CSCW) research, they are offered as ways to put colleagues in closer contact with each other by facilitating informal collaborative processes. However, they also pose questions about how users can exercise choice about who has access to this fine-grained personal information and how unwelcome intrusion can be avoided. Concerns about privacy become more acute with the prospect that these technologies are seen increasingly as forerunners of a wide range of commercial products aimed at mass markets.
At this stage in their evolution, full-featured MMCSs are to be found principally in experimental settings and the users so far are mainly people who are themselves contributing in some way to the development of the technologies. These laboratory settings provide an essential starting point for investigating the implications for employee privacy and the prospects for designs that appropriately respect users’ needs in this area.
This chapter reports on an exploratory study seeking to draw upon the experiences at the leading centers of MMCS research currently experimenting with multimedia communications. A central aim is to assist in developing a privacy framework applicable across a range of specific settings. A broader aim is explore the role that privacy considerations have and might play in the development of MMCS more generally. More personally, this research reflects a long-standing interest in the workplace implications of computerization. In particular, I am concerned that technological innovations too often disadvantage those in organizationally weak positions who have little influence on the course of development, while being among the first to experience any adverse effects (Clement, 1992, 1994). MMCS poses a potential threat in this regard.
The chapter begins with a summary of recent MMCS research and then discusses basic notions of privacy and how they relate generally to MMCS technology. The section following describes how researchers working on particular MMCS applications have reacted to privacy concerns in their immediate settings, and provides the basis for identifying general design principles that can be drawn from these experiences. The next section steps back from particular applications and examines how the research lab settings may influence the overall course of MMCS development. The chapter closes with suggestions for further development of the MMCS social/technological “genre.”
Brain computer interfaces for brain imaging may replace lost functions, such as speaking or moving. They may restore the ability to control the body, such as by stimulating nerves or muscles that move the hand. BCIs have also been used to improve functions, such as training users to improve the remaining function of damaged pathways required to grasp. BCIs can also enhance function, like warning a sleepy driver to wake up. Finally, a BCI might supplement the body’s natural outputs, such as through a third hand.
Different techniques are used to measure brain activity for BCIs. Most BCIs have used electrical signals that are detected using electrodes placed invasively within or on the surface of the cortex, or noninvasively on the surface of the scalp [electroencephalography (EEG)]. Some BCIs have been based on metabolic activity that is measured noninvasively, such as through functional magnetic resonance imaging (fMRI).
Many core systems is a system with hundreds or thousands of cores and implements parallel architecture
This chapter is focused on providing an overview of noninvasive BCIs. After a brief review of the relevant aspects of EEG and fMRI, each of the subsequent sections is dedicated to one of the four different purposes that a BCI may serve and that have been realized as of this writing.
Agent Oriented Software Engineering (AOSE) is a new software engineering paradigm that arose to apply best practice in the development of complex Multi Agent Systems (MAS) by focusing on the use of agents, and organizations (communities) of agents as the main abstractions. The field of Software Product Lines (SPL) covers all the software development lifecycle necessary to develop a family of products where the derivation of concrete products is made systematically and rapidly.
Many applications such as multimedia communications, video conferencing, distance learning, computer-supported collaborative work (CSCW), distributed games, and video-on-demand require multipoint group communications to enable the participation of multiple sources and receivers. This group communication can be achieved by sending a copy of the message to one receiver at a time, sending everyone the same message including to those who are not members of a group, or by selectively and simultaneously sending only to the group members. These approaches have been termed replicated unicasting, broadcasting or flooding, and multicasting, respectively. These can be illustrated by the example shown in Figure 16.1. If only A and C need to communicate, unicasting is just fine. If everyone in the network needs to communicate, broadcasting can be deployed. However, if only some of them, say A, B, and D, need to be part of a group, then the network should limit the traffic among these without broadcasting to everyone by using multicast communications. Multicasting is more efficient than replicated unicasting or broadcasting for multipoint group communications. However, incorporating the support for multicasting is difficult in current and emerging networks, as many of them are not designed to support multicast communications. There has been a significant amount of work toward supporting multicast communications in existing and emerging networks for the last several years. The support for multicasting in the Internet Protocol (IP) was first proposed in [Deering, 1989] and now has been included in many routers, hosts, and clients [Jamison, 1998]. Also, several solutions have been proposed for supporting multicast communications in emerging Asynchronous Transfer Mode (ATM) networks. Emerging wide-area networks such as the very-high-speed Backbone Network Service (vBNS) have built-in support for multicast communications. Attempts are also being made to extend the support for multicast communications in emerging wireless and mobile networks Besides being an interesting challenge to the research community, multicasting has also become a topic of great interest to Internet Service Providers (ISPs), competitive local exchange carriers (CLECs), content providers, and businesses with multiple sites requiring simultaneous updates. ISPs could use multicasting to support content distribution services and thus be able to charge more to their customers for this premium service. Businesses could use multicasting to distribute software and data updates to branch offices and stores worldwide [Lawton, 1998]. There are many important issues, technical and nontechnical, in multicasting that need to be addressed before multicasting can be widely deployed as shown in
Through the proliferation of technology in modern-day homes, people communicate in a variety of ways through multimedia. The term "multimedia" itself can take many forms, but it always relates to the way computers present different elements. Multimedia is common on a variety of kinds of websites and if you use a computer, you use multimedia communication daily.
Text communication encompasses a variety of forms and is one of the most common forms of multimedia communication in a computer user's day-to-day activities. Text communication includes such areas of Internet use as reading a website, reading and writing email messages and instant messaging. Text communication is also the oldest form of multimedia communication, as the first computers displayed text only. Image Communication
Though images might not seem to be a form of communication in the same way that text is a form of communication, it is a legitimate form of multimedia communication that many users enjoy daily. Examples include browsing an online photo album, opening and viewing images attached to an email and looking at photos that accompany stories on news websites.
Multimedia is the use of a computer to present and combine text, graphics, audio, and video with links and tools that let the user navigate, interact, create, and communicate. This definition contains four components essential to multimedia. First, there must be a computer to coordinate what you see and hear, and to interact with. Second, there must be links that connect the information. Third, there must be navigational tools that let you traverse the web of connected information. Finally, because multimedia is not a spectator sport, there must be ways for you to gather, process, and communicate your own information and ideas. If one of these components is missing, you do not have multimedia. For example, if you have no computer to provide interactivity, you have mixed media, not multimedia. If there are no links to provide a sense of structure and dimension, you have a bookshelf, not multimedia. If there are no navigational tools to let you decide the course of action, you have a movie, not multimedia. If you cannot create and contribute your own ideas, you have a television, not multimedia.
An important goal of this course is to enable you to become a creator, not just a consumer, of multimedia on the Internet. In this multimedia section of the Web Design portal, therefore, I will be putting resources requested by students who are working to design multimedia components to incorporate into their Web pages.
Audio Communication
A common form of Web-based multimedia communication is audio communication. This form involves receiving a message through an audio format, such as listening to an online radio station or playing a music file. If you use the Internet to stream a radio station broadcast, for example, you are engaging in a form of audio communication. Audio communication often combines with other forms of multimedia communication. A slideshow, for example, can feature text, images and audio together.
Video Communication
As its name indicates, video communication is a form of multimedia communication through video. It is common on many websites, including YouTube and the websites of television stations. Since high-speed Internet has become common, video communication has increased as users are able to access this form of multimedia communication. Types of video communication include .AVI, MPEG, WMV and QuickTime files.
Multimedia is the field concerned with the computer-controlled integration of text, graphics, drawings, still and moving images (Video), animation, audio, and any other media where every type of information can be represented, stored, transmitted and processed digitally.
A Multimedia Application is an Application which uses a collection of multiple media sources e.g. text, graphics, images, sound/audio, animation and/or video.
Hypermedia can be considered as one of the multimedia applications.



